<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
    ></script>
    <!--It is a link for MathJax-->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <!--link for the font-->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <!--link for the font-->
    <link
      href="https://fonts.googleapis.com/css2?family=Spectral:wght@200&display=swap"
      rel="stylesheet"
    />
    <!--link for the font-->
    <link rel="stylesheet" type="text/css" href="note_style.css" />
    <!--It is a link for css structure-->
    <script src="highlight.js"></script>
    <!--It is a link for highlight.js-->
    <title>Numerical Analysis</title>
  </head>
  <body>
    <div class="top-bar"></div>
    <input type="checkbox" id="nav-toggle" class="nav-toggle" />
    <label for="nav-toggle" class="icon-burger">
      <div class="line"></div>
      <div class="line"></div>
      <div class="line"></div>
      <span class="visually-hidden">Menu</span>
    </label>
    <nav class="navbar">
      <a href="#binary">Binary</a>
      <a href="#unit_roundoff">Unit Roundoff</a>
      <a href="#roundoff_error">Roundoff Error</a>
      <a href="#base">Number System with base \(\beta\)</a>
      <a href="#floating_point_error_analysis">Floating point error analysis</a>
      <a href="#orders_of_convergence">Orders of Convergence</a>
      <a href="#newton_method">Newton's Method</a>
      <a href="#error_analysis_newton">Error Analysis for Newton's Method</a>
      <a href="#theorems_on_newton_method">Theorems on Newton's Method</a>
      <a href="#horner_algorithm">Horner's Algorithm</a>
      <a href="#contractive_mapping_theorem">Contractive Mapping</a>
      <a href="#polynomial_interpretation">Polynomial Interpretation</a>
      <a href="#gaussian_quadrature">Gaussian Quadrature</a>
      <a href="#gaussian_quadrature_error_analysis">Error Analysis for Gaussian Quadrature</a>
      <a href="#composite_formulae">Composite Formulae</a>
      <a href="#theta_method">\(\theta\)-Method</a>
      <a href="#error">Error</a>
      <a href="#initial_value_problems">Initial Value Problem</a>
      <a href="#stability">Stability</a>
      <a href='#local_truncation_error_multistep'>Local Truncation Error for Multistep Methods</a>
      <a href="#positive_definite">Positive Definite</a>
      <a href="#gerschgorin_theorem">Gerschgorin Theorem</a>
      <a href="#rayleigh_quotient">Rayleigh Quotient</a>
      <a href="#january_2019">January 2019</a>
      <a href="#january_2020">January 2020</a>
      <a href="#august_2020">August 2020</a>
      <a href="#References">References</a>
    </nav>
    <script src="click.js"></script>
    <!--It is a link for click.js, which is clicking the link the hamburger menu will disappear-->
    <script src="mathjax_config.js"></script>
    <!--It is a link for Mathjax configuration-->

    <section>
      <p>
        My name is Hanzhang Yin, and I have developed this website as a resource to facilitate the review of key concepts in abstract algebra, and it is not fully completed.
        Feel free to email me at <a href="mailto:hanyin@ku.edu">hanyin@ku.edu</a> if there are any errors or suggestions for improvement.
      </p>
    </section>

    <h1>Numerical Analysis</h1>

    <section id="binary">
      <h2>Binary</h2>
      <p>
        \[ x = \pm (0. a_1\cdots a_n\cdots)_2\times 2^m \] where \(a_1 = 1\)
      </p>
      <p>
        \(\textbf{Note. }\)For Marc-32, \(n = 24\), \(-126\leq m\leq 127\). For
        Marc-64, \(n = 53\), \(-1021\leq m\leq 1024\).
      </p>
    </section>

    <br />

    <section id="unit_roundoff">
      <h2>Unit Roundoff</h2>
      <p>
        \[ fl(1 + \varepsilon) > 1 \] \[ \varepsilon = 2^{-24} ~ 10^{-8} \] \[
        \varepsilon = 2^{-53} ~ 10^{-16} \]
      </p>
    </section>

    <br />

    <section id="roundoff_error">
      <h2>Roundoff Error</h2>
      <p>
        Let \(\delta = \frac{x - fl(x)}{x}\). define \[ \left|\delta\right|\leq
        \varepsilon \] Suppose that \(x = \pm (0. a_1\cdots a_n\cdots)_2\times
        2^m\), then we pick two points such that \(x_-\) is less than \(x\) and
        \(x_+\) is greater than \(x\). Then we have \[ x_- = (0, a_1\cdots
        a_n)_2\times 2^m \] \[ x_+ = (0, a_1\cdots a_n+0.000\dots 1_n)_2\times
        2^m \] \[ fl(x) = x_- \text{ if }a_{n+1} = 0 \text{(x is closer to x_-
        than x_+) and } fl(x) = x_+ \text{ if }a_{n+1} = 1\text{ (x is closer to
        x_+ than x_-)} \] \[ |fl(x) - x|\leq \frac{1}{2}\times (x_+ - x_-) =
        \frac{1}{2}\cdot 2^{-n}\cdot 2^m \] \[ \left|\frac{fl(x) -
        x}{x}\right|\leq \frac{\frac{1}{2}2^{-n}2^m}{(0,a_1\cdots a_n)_2\times
        2^m} = 2^{-n} \]
      </p>
    </section>

    <br />

    <section id="base">
      <h2>Number System with base \(\beta\)</h2>
      <p>
        The unit roundoff is \[ \varepsilon = \frac{1}{2}\beta^{1-t} \] where
        \(t\) is the number of digits in the mantissa.
      </p>
    </section>

    <br />

    <section id="floating_point_error_analysis">
      <h2>Floating point error analysis</h2>
      <p>
        Let \(x, y\) be two machine numbers. Let \(\circ\) is a binary
        operation. \[ fl(x\circ y) = (x\circ y)(1 + \delta) \] where
        \(|\delta|\leq\varepsilon\) is the roundoff error. \[ fl(x) = x(1 +
        \delta) \]
      </p>
      <p>\(\textbf{Example.}\)</p>
    </section>

    <br />

    <section id = 'orders_of_convergence'>
      <h2>Orders of Convergence</h2>
      <p>
        Let \(\{x_n\}\) be a sequence of real numbers tending to a limit \( x^* \). We say that the rate of convergence is at least <b>linear</b> if there are a constant \( c < 1 \) and an integer \( N \) such that

        \[
        |x_{n+1} - x^*| \le c |x_n - x^*| \quad (n \ge N)
        \]

        We say that the rate of convergence is at least <b>superlinear</b> if there exist a sequence \( \epsilon_n \) tending to 0 and an integer \( N \) such that

        \[
        |x_{n+1} - x^*| \le \epsilon_n |x_n - x^*| \quad (n \ge N)
        \]

        The convergence is at least <b>quadratic</b> if there are a constant \( C \) (not necessarily less than 1) and an integer \( N \) such that

        \[
        |x_{n+1} - x^*| \le C |x_n - x^*|^2 \quad (n \ge N)
        \]

        In general, if there are positive constants \( C \) and \( \alpha \) and an integer \( N \) such that

        \[
        |x_{n+1} - x^*| \le C |x_n - x^*|^\alpha \quad (n \ge N)
        \]

        we say that the rate of convergence is of <b>order \(\alpha\)</b> at least.
      </p>

      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page. 17.</i></b>
      </p>
    </section>

    <section id="newton_method">
      <h2>Newton's Method</h2>
      <p>
        $\textbf{Definition. }$ Newton's method, also known as the
        Newton-Raphson method, is an iterative root-finding algorithm used to
        find successively better approximations to the roots (or zeros) of a
        real-valued function. Given a function \( f(x) \) and its derivative \(
        f'(x) \), Newton's method starts with an initial guess \( x_0 \) and
        iteratively refines this guess using the following formula: \[ x_{n+1} =
        x_n - \frac{f(x_n)}{f'(x_n)}, \] where:
      </p>
      <ul>
        <li>\( x_n \) is the current approximation.</li>
        <li>\( x_{n+1} \) is the next approximation.</li>
        <li>\( f(x_n) \) is the value of the function at \( x_n \).</li>
        <li>
          \( f'(x_n) \) is the value of the derivative of the function at \( x_n
          \).
        </li>
      </ul>
      <p>
        $\textbf{convergence Criteria.}$ The method typically converges if the
        initial guess \( x_0 \) is sufficiently close to the actual root, and \(
        f(x) \) is continuously differentiable in the neighborhood of the root.
      </p>
      <p>
        $\textbf{Example. }$Consider finding the root of the function \( f(x) =
        x^2 - 2 \). The derivative of the function is \( f'(x) = 2x \). Firstly,
        we start with an initial guess \( x_0 \). Secondly, we apply the
        Newton's iteration: \[ x_{n+1} = x_n - \frac{x_n^2 - 2}{2x_n} =
        \frac{2x_n - \frac{x_n^2 - 2}{x_n}}{2} = \frac{x_n + \frac{2}{x_n}}{2}
        \] So, the iterative formula becomes: \[ x_{n+1} = \frac{1}{2} \left(
        x_n + \frac{2}{x_n} \right) \] By repeatedly applying this formula, the
        sequence \( \{x_n\} \) will converge to the square root of 2.
      </p>
    </section>

    <br />

    <section id="error_analysis_newton">
      <h2>Error Analysis for Newton's Method</h2>
      <p>
        Let \( r \) be a solution of \( f(x) = 0 \) (i.e. \( f(r) = 0 \)).
        Suppose that we have already computed \( x_n \) and the error in \( x_n
        \) is \(e_n = |x_n - r| \). We now derive a formula that relates the
        error after the next step, \( |x_{n+1} - r| \), to \( |x_n - r| \).
        According to Taylor's theorem, there is a \( c \) between \( x_n \) and
        \( r \) such that \[ 0 = f(r) = f(x_n) + f'(x_n)(r - x_n) +
        \frac{1}{2}f''(c)(r - x_n)^2 \quad (1) \] By the definition of Newton's
        method, we have \[ 0 = f(x_n) + f'(x_n)(x_{n+1} - x_n) \quad (2) \]
        Subtracting (2) from (1). \[ \begin{align} 0 &= f'(x_n)(r - x_{n+1}) +
        \frac{1}{2}f''(c)(r - x_n)^2\\ x_{n+1} - r &=
        \frac{f''(c)}{2f'(x_n)}(x_n - r)^2 \\ e_{n+1} &= |x_{n+1} - r| = \left|
        \frac{f''(c)}{2f'(x_n)} \right||x_n - r|^2 \\ \end{align} \] If \( x_n
        \) is close to \( r \), then \( c \), which must be between \( x_n \)
        and \( r \), is also close to \( r \) and \[ e_{n+1} = |x_{n+1} - r|
        \approx \left| \frac{f''(r)}{2f'(r)} \right||x_n - r|^2 = \left|
        \frac{f''(r)}{2f'(r)}\right| e_n^2 = Ce_n^2 \]
      </p>
      <!-- Even if \( x_n \) is not close to \( r \), by the hypotheses (H1) and (H2) on the behavior of \( f \)
        \[ |x_{n+1} - r| \leq M|x_n - r|^2 \quad (3) \] -->
      <p>
        <b>
          Thus, we can see that Newton's Method is quadratically convergent.
        </b>
      </p>
      <p>
        <b>
          However, the disadvantage of Newton's Method is that if the initial
          value is closed enough to the root.
        </b>
      </p>
    </section>

    <br />

    <section id="theorems_on_newton_method">
      <h2>Theorems on Newton's Method</h2>
      <p>
        <strong>Theorem.</strong>
        Let \(f''\) be continuous and let \(r\) be a simple zero of \(f\). Then
        there is a neighborhood of \(r\) and a constant \(C\) such that if
        Newton's method is started in that neighborhood, the successive points
        become steadily closer to \(r\) and satisfy \[ |x_{n+1} - r| \leq C
        (x_n- r)^2 \quad (n \geq 0) \]
      </p>
      <p>
        $\textbf{Theorem on Newton's Method for a Convex Function. }$ If \( f \)
        belongs to \( C^2(\mathbb{R}) \), is increasing, is convex, and has a
        zero, then the zero is unique, and the Newton iteration will converge to
        it from any starting point.
      </p>
      <p>
        $\textbf{Convergence of Newton's method.}$ Assume that \( f(x) \), \(
        f'(x) \), and \( f''(x) \) are continuous for all \( x \) in some
        neighborhood of \( \alpha \) and \( f(\alpha) = 0 \) and \( f'(\alpha)
        \neq 0 \). If \( x_0 \) is sufficiently close to \( \alpha \), then \(
        x_n \to \alpha \) as \( n \to \infty \). Moreover, \[ \lim_{n \to
        \infty} \frac{|x_{n+1} - \alpha|}{|x_n - \alpha|^2} = \frac{1}{2} \left|
        \frac{f''(\alpha)}{f'(\alpha)} \right|. \]
      </p>
    </section>

    <br />

    <section id="horner_algorithm">
      <h2>Horner's Algorithm</h2>
      <p>
        This method is also known as Horner's method, nested multiplication, or
        synthetic division.
      </p>
      <p>
        \(\textbf{Definition. }\) Horner's algorithm is a method for evaluating
        a polynomial at a point. Given a polynomial \( p(x) = a_0 + a_1 x + a_2
        x^2 + \cdots + a_n x^n \), Horner's algorithm evaluates \( p(x) \) at a
        point \( x = c \) as follows: \[ p(c) = a_0 + c(a_1 + c(a_2 + \cdots +
        c(a_{n-1} + c a_n) \cdots)). \]
      </p>
      <p>
        Given a polynomial \( p(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x
        + a_0 \). Then \[ \begin{align} b_{n-1} &= a_n \\ b_{n-2} &= a_{n-1} +
        z_0 b_{n-1} \\ &\vdots \\ b_0 &= a_1 + z_0 b_1 \\ p(z_0) &= a_0 + z_0
        b_0 \end{align} \] Horner's algorithm evaluates \( p(x) \) at a point \(
        x = z_0 \) as follows: \[ \begin{array}{cccccc} & a_n & a_{n-1} &
        a_{n-2} & \cdots & a_0 \\ z_0 & & z_0 b_{n-1} & z_0 b_{n-2} & \cdots &
        z_0 b_0 \\ & b_{n-1} & b_{n-2} & b_{n-3} & \cdots & \boxed{b_{-1}} \\
        \end{array} \] The boxed number satisfies \(p(z_0) = b_{-1}\).
      </p>
      <p>
        \(\textbf{Example. }\) Given the polynomial \( p(x) = 4x^3 - 3x^2 + 2x -
        1 \) and \( z_0 = 2 \), evaluate \( p(z_0) \). \[ \begin{array}{ccccc} &
        4 & -3 & 2 & -1 \\ 2 & & 8 & 10 & 24 \\ & 4 & 5 & 12 & \boxed{23} \\
        \end{array} \] Hence, we have \( p(2) = 23 \).
      </p>
      <p>
        \(\textbf{Theorem on Horner's Method. }\) Let \( p(x) = a_n x^n + \cdots
        + a_1 x + a_0 \). Define pairs \((\alpha_j, \beta_j)\) for \( j = n,
        n-1, \ldots, 0 \) by the algorithm \[ \begin{cases} (\alpha_n, \beta_n)
        = (a_n, 0) \\ (\alpha_j, \beta_j) = (a_j + x \alpha_{j+1}, \alpha_{j+1}
        + x \beta_{j+1}) & (n-1 \ge j \ge 0) \end{cases} \] Then \(\alpha_0 =
        p(x)\) and \(\beta_0 = p'(x)\).
      </p>
    </section>

    <br />

    <section id = 'contractive_mapping_theorem'>
      <h2>Contractive Mapping</h2>
      <p>
        Newton's method and Steffensen's method are examples of procedures whereby a sequence of points is computed by a formula of the form
        \[
        x_{n+1} = F(x_n) \quad (n \geq 0) \tag*{(1)}
        \]
        The algorithm defined by such an equation is called <b>functional iteration</b>.
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page. 100.</i></b>
      </p>
        <p>
            \(\textbf{Definition. }\) A mapping \( F \) of a metric space \( (X, d) \)
            into itself is called a <b>contraction</b> if there exists a constant \(
            k \) with \( 0 \leq k < 1 \) such that \[ d(F(x), F(y)) \leq k d(x, y)
            \] for all \( x, y \) in \( X \).
        </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page. 101.</i></b>
      </p>
      <h2>Contractive Mapping Theorem</h2>
      <p>
        <b>Theorem (Contractive Mapping Theorem).</b>
        Let \( C \) be a closed subset of the real line.
        If \( F \) is a contractive mapping of \( C \) into \( C \), then \( F \) has a unique fixed point.
        Moreover, this fixed point is the limit of every sequence obtained from Equation (1) with a starting point \( x_0 \in C \).
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page. 102.</i></b>
      </p>
    </section>

    <br>

    <section id="polynomial_interpretation">
      <h2>Polynomial Interpretation</h2>
      <h3>Theorem on Polynomial Interpolation Error</h3>
      <p>
        \(\textbf{Theorem. }\)Let \( f \) be a function in \( C^{n+1}[a, b] \),
        and let \( p \) be the polynomial of degree at most \( n \) that
        interpolates the function \( f \) at \( n + 1 \) distinct points \( x_0,
        x_1, \ldots, x_n \) in the interval \([a, b]\). To each \( x \) in \([a,
        b]\) there corresponds a point \( \xi_x \) in \((a, b)\) such that \[
        f(x) - p(x) = \frac{1}{(n + 1)!} f^{(n+1)}(\xi_x) \prod_{i=0}^{n} (x -
        x_i) . \]
      </p>

      <br />

      <h3>Lagrange's Form</h3>
      <p>
        The Lagrange form of the interpolation polynomial is a method for
        constructing a polynomial that passes through a given set of points.
        Specifically, given a set of \( n+1 \) distinct data points \((x_0,
        y_0), (x_1, y_1), \ldots, (x_n, y_n)\), the Lagrange interpolation
        polynomial \( P(x) \) is defined as: \[ P(x) = \sum_{i=0}^{n} y_i
        \ell_i(x) \] where \( \ell_i(x) \) are the Lagrange basis polynomials
        given by: \[ \ell_i(x) = \prod_{\substack{0 \le j \le n \\ j \ne i}}
        \frac{x - x_j}{x_i - x_j} \] Each \( \ell_i(x) \) is a polynomial of
        degree \( n \) that is 1 at \( x = x_i \) and 0 at \( x = x_j \) for \(
        j \ne i \).
      </p>
      <p>
        \(\textbf{Example. }\) Given points: \( (1, 1), (2, 4), (3, 9) \). Here
        \(x_0 = 1, x_1 = 2, x_2 = 3\), and \(y_0 = 1, y_1 = 4, y_2 = 9\).
      </p>
      <p>
        Compute the Lagrange basis polynomials \( \ell_i(x) \): \[ \ell_0(x) =
        \frac{(x-2)(x-3)}{(1-2)(1-3)} = \frac{(x-2)(x-3)}{2} \] \[ \ell_1(x) =
        \frac{(x-1)(x-3)}{(2-1)(2-3)} = - (x-1)(x-3) \] \[ \ell_2(x) =
        \frac{(x-1)(x-2)}{(3-1)(3-2)} = \frac{(x-1)(x-2)}{2} \]
      </p>
      <p>
        Construct the interpolation polynomial \( P(x) \): \[ P(x) = 1 \cdot
        \ell_0(x) + 4 \cdot \ell_1(x) + 9 \cdot \ell_2(x). \] \[ P(x) = 1 \cdot
        \frac{(x-2)(x-3)}{2} - 4 \cdot (x-1)(x-3) + 9 \cdot
        \frac{(x-1)(x-2)}{2}. \] Simplifying, we get: \[ P(x) =
        \frac{(x-2)(x-3)}{2} - 4(x-1)(x-3) + \frac{9(x-1)(x-2)}{2}. \]
      </p>

      <br />

      <h3>Divided Differences</h3>
      <p>
        The divided differences of a set of data points are a sequence of
        numbers that are used to construct the Newton form of the interpolation
        polynomial.
      </p>
      <p>
        \[f[x_i, x_{i+1}, \ldots, x_{i+j}] = \frac{f[x_{i+1}, x_{i+2}, \ldots,
        x_{i+j}] - f[x_i, x_{i+1}, \ldots, x_{i+j-1}]}{x_{i+j} - x_i}\] The
        Following table shows the divided differences for a set of data points:
        \[\begin{array}{cccc} x_0 & f[x_0] & f[x_0, x_1] & f[x_0, x_1, x_2] &
        f[x_0, x_1, x_2, x_3] \\ x_1 & f[x_1] & f[x_1, x_2] & f[x_1, x_2, x_3] &
        \\ x_2 & f[x_2] & f[x_2, x_3] & & \\ x_3 & f[x_3] & & & \end{array} \]
      </p>
      <p>
        \(\textbf{Example. }\) Given the following table of data points: \[
        \begin{array}{c|cccc} x & 3 & 1 & 5 & 6 \\ \hline f(x) & 1 & -3 & 2 & 4
        \\ \end{array} \] Solution: We arrange the given table vertically and
        compute divided differences by the use of Formula above, arriving at \[
        \begin{array}{ccccc} 3 & 1 & 2 & -3/8 & 7/40 \\ 1 & -3 & 5/4 & 3/20 & \\
        5 & 2 & 2 & & \\ 6 & 4 & & & \\ \end{array} \]
      </p>
      <p>
        <b>Theorem on Permutations in Divided Differences.</b> The divided
        difference is a symmetric function of its arguments. Thus, if \((z_0,
        z_1, \ldots, z_n)\) is a permutation of \((x_0, x_1, \ldots, x_n)\),
        then \[ f[z_0, z_1, \ldots, z_n] = f[x_0, x_1, \ldots, x_n]. \]
      </p>
      <p>
        <b>Theorem on Derivatives and Divided Differences.</b> If \( f \)
        is \( n \) times continuously differentiable on \([a, b]\) and if \(
        x_0, x_1, \ldots, x_n \) are distinct points in \([a, b]\), then there
        exists a point \( \xi \) in \((a, b)\) such that \[ f[x_0, x_1, \ldots,
        x_n] = \frac{1}{n!} f^{(n)}(\xi). \]
      </p>

      <br />

      <h3>Newton's Form</h3>
      <p>
        The Newton form of the interpolation polynomial is a method for
        constructing a polynomial that passes through a given set of points.
        Specifically, given a set of \( n+1 \) distinct data points \((x_0,
        y_0), (x_1, y_1), \ldots, (x_n, y_n)\), the Newton interpolation
        polynomial \( P(x) \) is defined as: \[ P(x) = a_0 + a_1(x - x_0) +
        a_2(x - x_0)(x - x_1) + \cdots + a_n(x - x_0)(x - x_1) \cdots (x -
        x_{n-1}) \] where \( a_0, a_1, \ldots, a_n \) are the <i><b>divided
        differences</b></i> of the data points.
      </p>
      <p>\(\textbf{Remark. }\)Here, \(a_n = f[x_0, x_1, \ldots, x_n]\).</p>
      <p>
        \(\textbf{Example. }\) Given the following table of data points: \[
        \begin{array}{c|cccc} x & 3 & 1 & 5 & 6 \\ \hline f(x) & 1 & -3 & 2 & 4
        \\ \end{array} \] We already found the divided differences in the
        previous example. The Newton form of the interpolation polynomial is \[
        p(x) = 1 + 2(x - 3) - \frac{3}{8}(x - 3)(x - 1) + \frac{7}{40}(x - 3)(x
        - 1)(x - 5). \]
      </p>
    </section>

    <br />

    <section>
      <h2>Theorem on Polynomial Interpolation Error</h2>
      <p>
        \(\textbf{Theorem. }\)Let \( f \) be a function in \( C^{n+1}[a, b] \),
        and let \( p \) be the polynomial of degree at most \( n \) that
        interpolates the function \( f \) at \( n + 1 \) distinct points \( x_0,
        x_1, \ldots, x_n \) in the interval \([a, b]\). To each \( x \) in \([a,
        b]\) there corresponds a point \( \xi_x \) in \((a, b)\) such that \[
        f(x) - p(x) = \frac{1}{(n + 1)!} f^{(n+1)}(\xi_x) \prod_{i=0}^{n} (x -
        x_i) . \]
      </p>
    </section>

    <br>

    <section id="gaussian_quadrature">
      <h2>Gaussian Quadrature</h2>
      <p>
        The theory can be formulated for quadrature rules of a slightly more
        general form; namely, \[ \int_a^b f(x) w(x) \, dx \approx \sum_{i=0}^n
        A_i f(x_i), \] where \( w \) is a fixed positive <b>weight function</b>.
      </p>
      <p>
        The <b>Gauss quadrature</b> rule:
        \[
        \int_a^b w(x) f(x) \, dx \approx G_n(f) = \sum_{k=0}^{n} W_k f(x_k), \tag*{(10.6)}
        \]
        where the <b>quadrature weights</b> are
        \[
        W_k = \int_a^b w(x) [L_k(x)]^2 \, dx, \tag*{(10.7)}
        \]
        and the <b>quadrature points</b> \( x_k, \, k = 0, \ldots, n \), are chosen as the zeros of the polynomial of degree \( n + 1 \) from a system of orthogonal polynomials over the interval \( (a, b) \) with respect to the weight function \( w \). Since this quadrature rule was obtained by exact integration of the Hermite interpolation polynomial of degree \( 2n + 1 \) for \( f \), it gives the exact result whenever \( f \) is a polynomial of degree \( 2n + 1 \) or less.
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 279.</i></b>
      </p>
    </section>

    <br>

    <section id="gaussian_quadrature_error_analysis">
      <h2>Error Analysis for Gaussian Quadrature</h2>
      <p>
        Given the Gaussian quadrature formula \[ \int_a^b f(x) dx =
        \sum_{i=1}^{n} w_i f(x_i) + R_n(f), \] where \[ R_n(f) =
        \frac{(b-a)^{2n+1}(n!)^4}{(2n+1)[(2n)!]^3} f^{(2n)}(c). \]
      </p>
      <p>
        <b>Theorem.</b> Suppose that \( w \) is a weight function, defined, integrable, continuous and positive on \( (a,b) \), and that \( f \) is defined and continuous on \( [a,b] \); suppose further that \( f \) has a continuous derivative of order \( 2n+2 \) on \( [a,b] \), \( n \geq 0 \). Then, there exists a number \( \eta \) in \( (a,b) \) such that
        \[
        \int_a^b w(x)f(x)dx - \sum_{k=0}^n W_k f(x_k) = K_n f^{(2n+2)}(\eta), \tag{10.18}
        \]
        and
        \[
        K_n = \frac{1}{(2n+2)!} \int_a^b w(x) [\pi_{n+1}(x)]^2 dx.
        \]
        Consequently, the integration formula (10.6), (10.7) will give the exact result for every polynomial of degree \( 2n + 1 \).
      </p>
        <p>
            <b><i>"Introduction to Numerical Analysis", page. 282.</i></b>
        </p>
    </section>

    <br />

    <section id = "composite_formulae">
      <h2>Composite Formulae</h2>
      <p>
        $\textbf{Definition(Composite trapezium rule). }$ \[ \int_a^b f(x) \, dx
        \approx h \left[ \frac{1}{2} f(x_0) + f(x_1) + \cdots + f(x_{m-1}) +
        \frac{1}{2} f(x_m) \right]. \]
      </p>
      <p>
        <b><i>Introduction to Numerical Analysis page. 209.</i></b>
      </p>
      <p>
        <b>Definition (Composite Simpson rule).</b>
        \[ \int_a^b f(x) \, dx \approx \frac{h}{3} \left[ f(x_0) + 4f(x_1) +
        2f(x_2) + 4f(x_3) + \cdots + 2f(x_{2m-2}) + 4f(x_{2m-1}) + f(x_{2m})
        \right]. \]
      </p>
      <p>
        <b><i>Introduction to Numerical Analysis page. 210.</i></b>
      </p>
      <p>
        <b>Definition (Composite Midpoint Rule).</b> The composite midpoint
        rule is the composite Gauss formula with \(w(x) \equiv 1\) and \(n = 0\)
        defined by \[ \int_a^b f(x) \, dx \approx h \sum_{j=1}^m f\left(a +
        \left(j - \frac{1}{2}\right) h \right). \] This follows from the fact
        that when \(n = 0\) there is one quadrature point \(\xi_0 = 0\) in
        \((-1,1)\), which is at the midpoint of the interval, and the
        corresponding quadrature weight \(W_0\) is equal to the length of the
        interval \((-1,1)\), i.e., \(W_0 = 2\). It follows from (10.24) with \(n
        = 0\) and \[ C_0 = \int_{-1}^1 t^2 \, dt = \frac{2}{3} \] that the error
        in the composite midpoint rule is \[ E_{0,m} = \frac{(b-a)^3}{24m^2}
        f''(\eta), \] where \(\eta \in (a, b)\), provided that the function
        \(f\) has a continuous second derivative on \([a, b]\).
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 286.</i></b>
      </p>
    </section>

    <br />

    <section id="theta_method">
      <h2>\(\theta\)-Method</h2>
      <p>
        <b>Theta methods.</b> We consider methods of the form \[
        y_{n+1} = y_n + h [ \theta f(t_n, y_n) + (1 - \theta) f(t_{n+1},
        y_{n+1}) ], \quad n = 0, 1, \ldots, \] where \( \theta \in [0, 1] \) is
        a parameter: - If \( \theta = 1 \), we recover Euler's method. - If \(
        \theta \in (0, 1) \), then the theta method (3.3) is implicit: Each time
        step requires the solution of \( N \) (in general, nonlinear) algebraic
        equations for the unknown vector \( y_{n+1} \). - The choices \( \theta
        = 0 \) and \( \theta = \frac{1}{2} \) are known as

        <i>Backward Euler:</i>
        \[ y_{n+1} = y_n + h f(t_{n+1}, y_{n+1}), \] $\textit{Trapezoidal
        rule}$: \[ y_{n+1} = y_n + \frac{1}{2} h [ f(t_n, y_n) + f(t_{n+1},
        y_{n+1}) ]. \] Solution of nonlinear algebraic equations can be done by
        iteration. For example, for backward Euler, letting \( y_{n+1}^{[0]} =
        y_n \), we may use $\textit{Direct iteration}$: \[ y_{n+1}^{[j+1]} = y_n
        + h f(t_{n+1}, y_{n+1}^{[j]}); \] $\textit{Newton–Raphson}$: \[
        y_{n+1}^{[j+1]} = y_{n+1}^{[j]} - \left[ I - h \frac{\partial f(t_n,
        y_n)}{\partial y} \right]^{-1} \left[ y_{n+1}^{[j]} - y_n - h f(t_{n+1},
        y_{n+1}^{[j]}) \right]; \] $\textit{Modified Newton–Raphson}$: \[
        y_{n+1}^{[j+1]} = y_{n+1}^{[j]} - \left[ I - h \frac{\partial f(t_n,
        y_n)}{\partial y} \right]^{-1} \left[ y_{n+1}^{[j]} - y_n - h f(t_{n+1},
        y_{n+1}^{[j]}) \right]. \]
      </p>
    </section>

    <br />

    <section id="error">
      <h2>Error</h2>
      <p>
        In numerically solving a differential equation, several types of errors
        arise. These are conveniently classified as follows:
      </p>
      <ul>
        <li>Local truncation error</li>
        <li>Local roundoff error</li>
        <li>Global truncation error</li>
        <li>Global roundoff error</li>
        <li>Total error</li>
      </ul>
      <p>
        <b>Local Truncation Error.</b> The local truncation error is the
        error made in a single step of a numerical method.
      </p>
      <p>
        <b>{Local Roundoff Error.</b> The local roundoff error is the error
        made in a single step of a numerical method due to roundoff.
      </p>
      <p>
        <b>Global Truncation Error.</b> The global truncation error is the
        accumulation of the local truncation errors in the previous steps.
      </p>
      <p>
        <b>Global Roundoff Error.</b> The global roundoff error is the
        accumulation of the local roundoff errors in the previous steps.
      </p>
      <p>
        <b>Total Error.</b> The total error is the sum of the global
        truncation error and the global roundoff error.
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page. 533.</i></b>
      </p>
    </section>

    <br />

    <section id="richardson_extrapolation">
      <h2>Richardson Extrapolation</h2>
      <p>
        \(\textbf{Definition. }\) Richardson extrapolation is a method for
        improving the accuracy of a numerical method. Given a numerical method
        that approximates a quantity with error \( E(h) \) for a step size \( h
        \), Richardson extrapolation constructs a new approximation with error
        \( E(h^2) \) by combining two approximations with step sizes \( h \) and
        \( h/2 \).
      </p>
    </section>

    <br>

    <section id="initial_value_problems">
      <h2>Initial value problems for ODEs</h2>
      <p>
        Our model is an initial-value problem written in the form \[
        \begin{cases} x' = f(t, x) \\ x(t_0) = x_0 \quad \tag{1} \end{cases} \]
        Here \( x \) is an unknown function of \( t \) that we hope to construct
        from the information given in Equations (1), where \( x' =
        \frac{dx(t)}{dt} \). The second of the two equations in (1) specifies
        one particular value of the function \( x(t) \). The first equation
        gives the slope.
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page.524.</i></b>
      </p>
      <h3>Existence</h3>
      <p>
        $\textbf{First Existence Theorem, Initial-Value Problem}$ If \( f \) is
        continuous in a rectangle \( R \) centered at \( (t_0, x_0) \), say \[ R
        = \{ (t, x) : |t - t_0| \leq \alpha, \quad |x - x_0| \leq \beta
        \}\tag{4} \] then the initial-value problem above has a solution \( x(t)
        \) for \( |t - t_0| \leq \min(\alpha, \beta / M) \), where \( M \) is
        the maximum of \( |f(t, x)| \) in the rectangle \( R \).
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page 525.</i></b>
      </p>
      <p>
        $\textbf{Second Existence Theorem, Initial-Value Problem.}$ If \( f \)
        is continuous in the strip \( a \leq t \leq b, -\infty < x < \infty \)
        and satisfies there an inequality \[ |f(t, x_1) - f(t, x_2)| \leq L |x_1
        - x_2| \tag{5} \] then the initial-value problem <b>(1)</b> has a unique
        solution in the interval \([a, b]\).
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing" page.526.</i></b>
      </p>
      <h3>Lipschitz Condition</h3>
      <p>
        <b>Definition</b>: A function \( f : \mathbb{R}^n \to \mathbb{R} \)
        satisfies the Lipschitz condition on a domain \( D \subset \mathbb{R}^n
        \) if there exists a constant \( L \geq 0 \) such that for all \( x, y
        \in D \): \[ \| f(x) - f(y) \| \leq L \| x - y \| \] where \( \| \cdot
        \| \) denotes a norm (commonly the Euclidean norm). And \( L \) is
        called the Lipschitz constant.
      </p>
      <p>
        <b>Note.</b> Hence, inequality <b>(5)</b> is the Lipschitz condition.
      </p>
      <h3>Uniqueness</h3>
      <p>
        <strong>Uniqueness Theorem, Initial-Value Problem.</strong>
        If \( f \) and \(\partial f / \partial x \) are continuous in the
        rectangle \( R \) defined by $\textbf{(4)}$, then the initial-value
        problem (1) has a unique solution in the interval \( |t - t_0| <
        \min(\alpha, \beta / M) \).
      </p>
      <p>
        $\textbf{Theorem (Picard's Theorem).}$ Suppose that the real-valued
        function \((x,y) \mapsto f(x,y)\) is continuous in the rectangular
        region \(D\) defined by \(x_0 \leq x \leq X_M\), \(y_0 - C \leq y \leq
        y_0 + C\); that \(|f(x,y_0)| \leq K\) when \(x_0 \leq x \leq X_M\); and
        that \(f\) satisfies the Lipschitz condition: there exists \(L > 0\)
        such that \[ |f(x,u) - f(x,v)| \leq L|u - v| \quad \text{for all} \quad
        (x,u) \in D, \quad (x,v) \in D. \] Assume further that \[ C \geq
        \frac{K}{L} \left( e^{L(X_M - x_0)} - 1 \right). \tag{12.3} \] Then,
        there exists a unique function \(y \in C^1[x_0, X_M]\) such that
        \(y(x_0) = y_0\) and \(y' = f(x,y)\) for \(x \in [x_0, X_M]\); moreover,
        \[ |y(x) - y_0| \leq C, \quad x_0 \leq x \leq X_M. \]
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 311.</i></b>
      </p>
      <h3>One-step methods</h3>
      <p>
        <b>Definition.</b> Generally, a one-step method may be written in the form

        \[
        y_{n+1} = y_n + h \Phi (x_n, y_n; h), \quad n = 0, 1, \ldots, N-1,\qquad y(x_0) = y_0, \tag*{(12.13)}
        \]

        where \(\Phi (\cdot, \cdot; \cdot)\) is a continuous function of its variables.
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 317.</i></b>
      </p>
      <p>
        <b>Definition.</b> In order to find the accuracy of the numerical method <b>(12.13)</b>, we define the <b>global error</b>, \( e_n \), by
        \[
        e_n = y(x_n) - y_n.
        \]
      </p>
        <p>
        <b>Definition.</b> The <b>truncation error</b>, \( T_n \), is defined by

        \[
        T_n = \frac{y(x_{n+1}) - y(x_n)}{h} - \Phi(x_n, y(x_n); h).  \tag*{(12.14)}
        \]
        </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 317.</i></b>
      </p>
      <p>
        <b>Theorem. </b> Consider the general one-step method <b>(12.13)</b> where, in addition to being a continuous function of its arguments, \(\Phi\) is assumed to satisfy a Lipschitz condition with respect to its second argument, that is, there exists a positive constant \( L_{\Phi} \) such that, for \( 0 \le h \le h_0 \) and for all \((x, u)\) and \((x, v)\) in the rectangle

        \[
        D = \{(x, y): x_0 \le x \le X_M, |y - y_0| \le C\},
        \]

        we have that

        \[
        |\Phi(x, u; h) - \Phi(x, v; h)| \le L_{\Phi} |u - v|. \quad (12.15)
        \]

        Then, assuming that \( |y_n - y_0| \le C \), \( n = 1, 2, \ldots, N \), it follows that

        \[
        |e_n| \le \frac{T}{L_{\Phi}} \left( e^{L_{\Phi} (x_n - x_0)} - 1 \right), \quad n = 0, 1, \ldots, N, \quad (12.16)
        \]

        where \( T = \max_{0 \le n \le N-1} |T_n| \).
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 318.</i></b>
      </p>
      <p>
        $\textbf{Euler's method.}$ Given that \( y(x_0) = y_0 \), let us suppose
        that we have already calculated \( y_n \), up to some \( n \), \( 0 \leq
        n \leq N - 1 \), \( N \geq 1 \); we define \[ y_{n+1} = y_n + h f(x_n,
        y_n). \] Thus, taking in succession \( n = 0, 1, \ldots, N - 1 \), one
        step at a time, the approximate values \( y_n \) at the mesh points \(
        x_n \) can be easily obtained. This numerical method is known as Euler's
        method.
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 317.</i></b>
      </p>

      <br>

      <h3>Consistency</h3>
      <p>
        <b>Definition.</b> The numerical method (12.13) is consistent with the differential equation (12.1) if the truncation error, defined by (12.14), is such that for any \( \epsilon > 0 \) there exists a positive \( h(\epsilon) \) for which \( |T_n| < \epsilon \) for \( 0 < h < h(\epsilon) \) and any pair of points \((x_n, y(x_n)), (x_{n+1}, y(x_{n+1}))\) on any solution curve in \( D \).
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 321.</i></b>
      </p>

      <p>
        <b>Theorem.</b> Suppose that the initial value problem (12.1), (12.2) satisfies the conditions of Picard's Theorem, and also that its approximation generated from (12.13) when \( h \le h_0 \) lies in the region \( D \). Assume further that the function \(\Phi (\cdot, \cdot; \cdot)\) is continuous on \( D \times [0, h_0] \), and satisfies the consistency condition (12.21) and the Lipschitz condition

        \[
        |\Phi(x, u; h) - \Phi(x, v; h)| \le L_{\Phi} |u - v| \quad \text{on} \quad D \times [0, h_0]. \tag*{(12.22)}
        \]

        Then, if successive approximation sequences \((y_n)\), generated by using the mesh points \( x_n = x_0 + nh \), \( n = 1, 2, \ldots, N \), are obtained from (12.13) with successively smaller values of \( h \), each \( h \) less than \( h_0 \), we have convergence of the numerical solution to the solution of the initial value problem in the sense that

        \[
        \lim_{n \to \infty} y_n = y(x) \quad \text{as} \quad x_n \to x \in [x_0, X_M] \quad \text{when} \quad h \to 0 \quad \text{and} \quad n \to \infty.
        \]
      </p>
      <p>
        <b><i>"Introduction to Numerical Analysis", page. 322.</i></b>
      </p>
      <h2>Adams-Bashforth Formula</h2>
      <p>
        Suppose the resulting formula is of the following type: \[ x_{n+1} = x_n + a\cdot f_n + b\cdot f_{n-1} + c\cdot f_{n-2} + \cdots \] where \( f_i \) denotes \( f(t_i, x_i) \). An equation of this type is called an
        <b>Adams-Bashforth formula</b>.
      </p>
      <p>
        <b>Note.</b> The Adams-Bashforth formula is an explicit method since it does not include $f_{n+1}$.
      </p>
      <h3>Adams-Bashforth Formula of Order $5$</h3>
      <p>
        \[x_{n+1} = x_n + \frac{h}{720} \left[ 1901 f_n - 2774 f_{n-1} + 2616 f_{n-2} - 1274 f_{n-3} + 251 f_{n-4} \right]\]
      </p>
      <h2>Adams-Moulton Formula</h2>
      <p>
        <b>Definition.</b> Suppose the resulting formula is of the following
        type: \[ x_{n+1} = x_n + a\cdot f_{n+1} + b\cdot f_{n} + c\cdot f_{n} +
        \cdots \] where \( f_i \) denotes \( f(t_i, x_i) \). An equation of this
        type is called an <b>Adams-Moulton formula</b>.
      </p>
      <p>
        <b>Note.</b> The Adams-Moulton formula is an implicit method since it includes \( f_{n+1} \).
      </p>
      <h3>Adams-Moulton Formula of Order $5$</h3>
      <p>
        \[ x_{n+1} = x_n + \frac{h}{720} \left[ 251 f_{n+1} + 646 f_n - 264 f_{n-1} + 106 f_{n-2} - 19 f_{n-3} \right] \]
      </p>
      <p>
        <b
          ><i
            >"Numerical Analysis: Mathematics of Scientific Computing", page.
            551.</i
          ></b
        >
      </p>
      <h2>Linear Multistep Methods</h2>
      <h3>Linear $k$-step method</h3>
      <p>
        The format of any such method is
        \[ a_k x_n + a_{k-1} x_{n-1} + \cdots + a_0 x_{n-k} = h \left[ b_k f_n + b_{k-1} f_{n-1} + \cdots + b_0 f_{n-k} \right]. \tag{9}\]
        Or in general, it can be written as \[\sum_{j=0}^{k} a_j
        x_{n+j} = h \sum_{j=0}^{k} b_j f(t_{n+j}, x_{n+j}), \] where \( a_k \neq
        0 \). This is called a \( k \)-step method.
      </p>
      <p>
        <b>Note.</b> The left side of the equation <b>(9)</b> is called the corresponding <i><b>characteristic polynomial</b></i> of the method.
      </p>
      <p><b>Note</b></p>
      <ul>
        <li>The coefficients \( a_i \) and \( b_i \) are given.</li>
        <li>
          \( x_i \) denotes an approximation to the solution at \( t_i \), with
          \( t_i = t_0 + ih \).
        </li>
        <li>The letter \( f_i \) denotes \( f(t_i, x_i) \).</li>
        <li>
          The formula above is used to compute \( x_n \), assuming that \( x_0,
          x_1, \ldots, x_{n-1} \) are already known (Assume that \( a_k \neq 0
          \)).
        </li>
        <li>If \( b_k = 0 \), the method is said to be explicit.</li>
        <li>If \( b_k \neq 0 \), the method is then said to be implicit.</li>
      </ul>
      <p>
        <b>Examples.</b>
      </p>
      <ul>
        <li>
          <b>The implicit Euler method</b> \(y_{n+1} = y_n + h f(x_{n+1}, y_{n+1})\) is an implicit linear one-step method.
        </li>
        <li>
          <b>The trapezium rule method</b> \(y_{n+1} = y_n + \frac{1}{2} h (f_{n+1} + f_n) \) is an implicit linear one-step method.
        </li>
        <li>
          <b>The Adams-Bashforth method</b> \(y_{n+4} = y_{n+3} + \frac{1}{24} h \left( 55 f_{n+3} - 59 f_{n+2} + 37 f_{n+1} - 9 f_n \right)\) is an
          example of an explicit linear four-step method.
        </li>
        <li>
          <b>The Adams-Moulton method</b> \(y_{n+3} = y_{n+2} + \frac{1}{24} h \left( 9 f_{n+3} + 19 f_{n+2} - 5 f_{n+1} - 9 f_n \right)\) is an
          implicit linear three-step method.
        </li>
      </ul>
      <p><i>"Introduction to Numerical Analysis" page. 331.</i></p>
      <h3>Order</h3>
      <p>
        The accuracy of a numerical solution of a differential equation is largely determined by the order of the algorithm used.
      </p>
      <p>
        <b>Definition.</b>The order indicates how many terms in a Taylor-series solution are being simulated by the method.
      </p>
      <p>
        For example, the Adams-Bashforth method: $x_{n+1} = x_n + \frac{h}{720}
        \left[ 1901 f_n - 2774 f_{n-1} + 2616 f_{n-2} - 1274 f_{n-3} + 251
        f_{n-4} \right]$ is said to be of order 5 because it produces
        approximately the same accuracy as the Taylor-series method with terms
        \( h, h^2, h^3, h^4, \) and \( h^5 \). The error can then be expected to
        be \( \mathcal{O}(h^6) \) in each step of a solution using the above
        method.
      </p>
      <p>
        In order to have better understanding of the order of the Multistep methods, we will define the following function
        \[ Lx = \sum_{i=0}^{k} \left[ a_i x(ih) - h b_i x'(ih) \right] a. \]
        However, in the following analysis, we assume that \( x \) is represented by its Taylor series at \( t = 0 \). By using the Taylor series for \( x \), one can express \( L \) in this form:

        \[ Lx = d_0 x(0) + d_1 h x'(0) + d_2 h^2 x''(0) + \cdots \tag{11} \]

        To compute the coefficients, \( d_i \), in Equation (11), we write the Taylor series for \( x \) and \( x' \):

        \[ x(ih) = \sum_{j=0}^{\infty} \frac{(ih)^j}{j!} x^{(j)}(0) \]

        \[ x'(ih) = \sum_{j=0}^{\infty} \frac{(ih)^j}{j!} x^{(j+1)}(0) \]
        These series are then substituted into Equation <b>(10)</b>. Rearranging the result in powers of \( h \), we obtain an equation having the form of <b>(11)</b>, with these values of \( d_i \):
        \[
        \begin{aligned}
        d_0 &= \sum_{i=0}^{k} a_i \\
        d_1 &= \sum_{i=0}^{k} (i a_i - b_i) \\
        d_2 &= \sum_{i=0}^{k} \left( \frac{1}{2} i^2 a_i - i b_i \right) \\
        &\vdots \\
        d_j &= \sum_{i=0}^{k} \left( \frac{i^j}{j!} a_i - \frac{i^{j-1}}{(j-1)!} b_i \right) \quad (j \geq 1)
        \end{aligned} \tag{12}
        \]
        Here we use \( 0! = 1 \) and \( i^0 = 1 \).
      </p>
      <p>
        <b>Theorem on Multistep Method Properties.</b>
        These three properties of the multistep method <b>(9)</b> are equivalent:
      </P>
      <ol>
        <li>\( d_0 = d_1 = \cdots = d_m = 0 \)</li>
        <li>\( Lp = 0 \) for each polynomial \( p \) of degree \( \leq m \)</li>
        <li>\( Lx \) is \( \mathcal{O}(h^{m+1}) \) for all \( x \in C^{m+1} \)</li>
      </ol>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing", page. 552 & 553.</i></b>
      </p>
      <h3>Alternate Definition of Order</h3>
      <p>
        Now, we can define the order of the multistep method as follows:
        The order of the multistep method in Equation <b>(9)</b> is the unique natural number \( m \) such that
        \[ d_0 = d_1 = \cdots = d_m = 0 \neq d_{m+1}. \]
      </p>
      <p>
        <b>EXAMPLE 1</b> What is the order of the method described by the following equation?

        \[ x_n - x_{n-2} = \frac{1}{3}h(f_n + 4f_{n-1} + f_{n-2}). \]

        <b>Solution.</b> Firstly, we rewrite the equation in the form of Equation <b>(9)</b>:
        \[
        \begin{align}
        x_n - x_{n-2} &= \frac{1}{3}h(f_n + 4f_{n-1} + f_{n-2}) \\
        x_n + 0\cdot x_{n-1} - x_{n-2} &= h\left(\frac{1}{3}f_n + \frac{4}{3}f_{n-1} + \frac{1}{3}f_{n-2}\right)
        \end{align}
        \]
        And we have to match it to
        \[ a_2 x_n + a_{1} x_{n-1} + a_0 x_{n-2} = h \left[ b_2 f_n + b_{1} f_{n-1} + b_0 f_{n-2} \right]. \]
        The vector \((a_0, a_1, a_2)\) is \((-1, 0, 1)\) and the vector \((b_0, b_1, b_2)\) is \(\left(\frac{1}{3}, \frac{4}{3}, \frac{1}{3}\right)\). Thus, the \( d_i \) are
        \[
        \begin{align}
        d_0 &= a_0 + a_1 + a_2 = 0 \\
        d_1 &= -b_0 + (a_1 - b_1) + (2a_2 - b_2) = 0 \\
        d_2 &= \left( \frac{1}{2}a_1 - b_1 \right) + (2a_2 - 2b_2) = 0 \\
        d_3 &= \left( \frac{1}{6}a_1 - \frac{1}{2}b_1 \right) + \left( \frac{4}{3}a_2 - 2b_2 \right) = 0 \\
        d_4 &= \left( \frac{1}{24}a_1 - \frac{1}{6}b_1 \right) + \left( \frac{2}{3}a_2 - \frac{4}{3}b_2 \right) = 0 \\
        d_5 &= \left( \frac{1}{120}a_1 - \frac{1}{24}b_1 \right) + \left( \frac{4}{15}a_2 - \frac{2}{3}b_2 \right) = -\frac{1}{90}
        \end{align}
        \]
        The order of the method is \(4\).
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing", page 554.</i></b>
      </p>
    </section>

    <section id="stability">
      <h2>Root Condition</h2>
      <p>
        <b>Definition.</b> The roots of the first characteristic polynomial lie in the closed unit disc and those on the unit circle are simple is often called the Root Condition.
      </p>
      <h2>Stability</h2>
      <p>
        <b>Definition.</b> The roots of $\rho(z)$ of modulus one are called essential roots.
        The root \( z = 1 \) is called the principal root.
        The roots of \(\rho(z)\) of modulus \( \lt 1 \) are called nonessential roots.
      </p>
      <h3>Weakly Stability</h3>
      <p>
        <b>Definition.</b> A linear multistep method is strongly stable if all roots of \(\rho(z)\) are \(\leq 1\) in magnitude and only one root has magnitude one.
        If more than one root has magnitude one, the method is called weakly or conditionally stable.
      </p>
      <p>
        <b>Note.</b> We still require only simple roots of magnitude one. Also, note these definitions refer to the case \( h = 0 \).
      </p>
      <h3>Strong Stability</h3>
      <p>
        <b>Definition.</b> A multistep method is said to be <i><b>strongly stable</b></i> if \( p(1) = 0, p'(1) \neq 0 \), and all other roots of \( p \) satisfy the inequality \( |z| < 1 \).
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing", page 564.</i></b>
      </p>
      <h3>Absolute Stability</h3>
      <p>
        <b>Definition.</b> A linear multistep method is said to be <i><b>absolutely stable</b></i> for a given value of \( \lambda h \) if each root \(z_r = z_r(\lambda h)\) of the associated stability polynomial \(\pi(\cdot; \lambda h)\) satisfies \(|z_r(\lambda h)| < 1\).
      </p>
      <h3>Region of Absolute Stability</h3>
      <p>
        <b>Definition.</b> The region of absolute stability of a linear multistep method is the set of all points \(\lambda h\) in the complex plane for which the method is absolutely stable.
      </p>
      <h3>\(A\)-stable</h3>
        <p>
          <b>Definition.</b> A linear multistep method is said to be <i><b>A-stable</b></i> if its region of absolute stability contains the negative (left) complex half-plane.
        </p>
      <p>
        <b>Second Barrier Theorem</b>:
      </p>
        <ul>
            <li>No explicit linear multistep method is A-stable.</li>
            <li>No A-stable linear multistep method can have order greater than 2.</li>
            <li>The second-order A-stable linear multistep method with the smallest error constant is the trapezium rule method.</li>
        </ul>
        <p>
            <b><i>"Introduction to Numerical Analysis", page 348.</i></b>
        </p>
    </section>

    <br>

    <section id = "local_truncation_error_multistep">
      <h2>Local Truncation Error, Multistep Method</h2>
      <p>
        <b>Theorem (Theorem on Local Truncation Error, Multistep Method).</b>

        If the multistep method (2) is of order \( m \), if \( x \in C^{m+2} \), and if \( \partial f / \partial x \) is continuous, then under the hypotheses of the preceding paragraph,
        \[
        x(t_n) - x_n = \left( \frac{d_{m+1}}{a_k} \right) h^{m+1} x^{(m+1)}(t_{n-k}) + \mathcal{O}(h^{m+2}) \tag*{(13)}
        \]
        (The coefficients \( d_k \) are defined in Section 8.4, p. 553.)
      </p>
      <p>
        <b><i>"Numerical Analysis: Mathematics of Scientific Computing", page 560.</i></b>
      </p>
    </section>

    <br>

    <section id="gram_schmidt">
      <h2>Gram-Schmidt</h2>
      <p></p>
    </section>

    <section id="qr_factorization">
      <h2>QR Factorization</h2>
      <h3>Reduced QR Factorization</h3>
      <p>
        <b>Definition [Reduced QR factorization].</b> Let \( A \in
        \mathbb{F}^{m \times n} \) (\( m \geq n \)). There is an orthonormal
        matrix \( \hat{Q} \in \mathbb{F}^{m \times n} \) and a square upper
        triangular matrix \( \hat{R} \in \mathbb{F}^{n \times n} \) such that \[
        A = \hat{Q} \hat{R}. \]
      </p>
      <p>There are several ways to compute the QR factorization of a matrix.</p>
      <ul>
        <li>
          <b>Gram-Schmidt Process.</b> The Gram-Schmidt process is a method for
          orthonormalizing a set of vectors. Given a set of linearly independent
          vectors \( \{v_1, v_2, \ldots, v_n\} \), the Gram-Schmidt process
          produces an orthonormal set of vectors \( \{q_1, q_2, \ldots, q_n\} \)
          such that \( \text{span}\{v_1, v_2, \ldots, v_n\} = \text{span}\{q_1,
          q_2, \ldots, q_n\} \).
        </li>
        <li>
          <b>Householder Transformation.</b> The Householder
          transformation is a reflection across a plane. Given a vector \( x \), the Householder transformation \( H \) is defined as
          \[ H = I - 2 \frac{vv^*}{v^*v}, \]
          where \( v = x - \text{sign}(x_1) \|x\|_2 e_1 \) and \( e_1 \) is the first unit vector.
        </li>
        <li>
          <b>Givens Rotation.</b> The Givens rotation is a rotation in the plane spanned by two vectors.
          Given a vector \( x \), the Givens rotation \( G \) is defined as
          \[ G = \begin{bmatrix} c & -s \\ s & c \end{bmatrix}, \]
          where \( c = \frac{x_1}{\|x\|_2} \) and \( s = \frac{x_2}{\|x\|_2} \).
        </li>
      </ul>
    </section>

    <section id="bauer-fike_Theorem">
      <h2>Bauer-Fike Theorem</h2>
      <p>
        Consider a diagonalizable matrix \(A \in \mathbb{C}^{n \times n}\) with
        non-singular eigenvector matrix \(V \in \mathbb{C}^{n \times n}\), such
        that \(A = V\Lambda V^{-1}\), where \(\Lambda\) is a diagonal matrix. If
        \(X \in \mathbb{C}^{n \times n}\) is invertible, its condition number in
        the \(p\)-norm is denoted by \(\kappa_p(X)\) and defined as: \[
        \kappa_p(X) = |X|_p |X^{-1}|_p \]
      </p>
      <p>
        \(\textbf{Theorem. }\) Let A be an \( n \times n \) matrix with a
        complete set of linearly independent eigenvectors and suppose the \(
        V^{-1}AV = D \) where \( V \) is non-singular and \( D \) is diagonal.
        Let \( \delta A \) be a perturbation of \( A \) and let \( \mu \) be an
        eigenvalue of \( A + \delta A \). Then \( A \) has an eigenvalue \(
        \lambda \) such that \[ |\mu - \lambda| \leq \kappa_p(V) \| \delta A
        \|_p, \quad 1 \leq p \leq \infty \] where \( \kappa_p(V) \) is the \( p
        \)-norm condition number of \( V \).
      </p>
      <p>The Bauer-Fike Theorem:</p>
      <ul>
        <li>Suppose \(\mu\) is an eigenvalue of \(A + \delta A\).</li>
        <li>
          Then there exists \(\lambda \in \Lambda(A)\) (i.e., an eigenvalue of
          \(A\)) such that: \[ |\lambda - \mu| \leq \kappa_p(V) \|\delta A\|_p
          \]
        </li>
        <li>Here, \(\|\cdot\|_p\) denotes the \(p\)-norm of a matrix.</li>
      </ul>
      <p>
        The <strong>Bauer–Fike theorem</strong> is a powerful result, and its
        proof involves several key concepts from linear algebra and perturbation
        theory. Let's outline the main steps of the proof:
      </p>

      <ul>
        <li>
          <strong>Eigenvalue Perturbation</strong>:
          <ul>
            <li>
              Consider a matrix \(A\) with eigenvalues \(\lambda_1, \lambda_2,
              \ldots, \lambda_n\).
            </li>
            <li>
              Suppose we have a perturbed matrix \(A + \delta A\), where
              \(\delta A\) represents a small perturbation.
            </li>
            <li>
              We want to understand how the eigenvalues of \(A + \delta A\)
              relate to the eigenvalues of \(A\).
            </li>
          </ul>
        </li>
        <li>
          <strong>Eigenvector Matrix and Diagonalization</strong>:
          <ul>
            <li>
              Recall that \(A\) can be diagonalized as \(A = V\Lambda V^{-1}\),
              where:
              <ul>
                <li>\(V\) is the matrix of eigenvectors of \(A\).</li>
                <li>
                  \(\Lambda\) is the diagonal matrix with eigenvalues
                  \(\lambda_1, \lambda_2, \ldots, \lambda_n\) on the diagonal.
                </li>
              </ul>
            </li>
            <li>The eigenvector matrix \(V\) is invertible.</li>
          </ul>
        </li>
        <li>
          <strong>Bounding the Deviation</strong>:
          <ul>
            <li>Let \(\mu\) be an eigenvalue of \(A + \delta A\).</li>
            <li>
              We want to find a bound on \(| \lambda - \mu |\), where
              \(\lambda\) is an eigenvalue of \(A\).
            </li>
            <li>
              Using the eigenvector matrix \(V\), we can express \(\mu\) as: \[
              \mu = v_i^T (A + \delta A) v_i = \lambda_i + v_i^T \delta A v_i \]
              Here, \(v_i\) corresponds to the \(i\)-th column of \(V\).
            </li>
            <li>
              Since \(\delta A\) is small, we can bound the deviation: \[ |
              \lambda - \mu | = | \lambda_i - \mu | = | v_i^T \delta A v_i |
              \leq \| v_i \|_p \| \delta A \|_p \] The inequality follows from
              the properties of matrix norms.
            </li>
          </ul>
        </li>
        <li>
          <strong>Condition Number of \(V\)</strong>:
          <ul>
            <li>
              The condition number of \(V\) in the \(p\)-norm is denoted by
              \(\kappa_p(V)\).
            </li>
            <li>
              It measures how sensitive the eigenvectors are to perturbations.
            </li>
            <li>
              We have: \[ | \lambda - \mu | \leq \kappa_p(V) \| \delta A \|_p \]
            </li>
          </ul>
        </li>
        <li>
          <strong>Alternate Formulation</strong>:
          <ul>
            <li>
              If we have an approximate eigenvalue-eigenvector pair
              \((\lambda_a, v_a)\), we can bound the error: \[ | \lambda -
              \lambda_a | \leq \frac{\kappa_p(V) \| r \|_p}{\| v_a \|_p} \]
              where \(r = Av_a - \lambda_av_a\).
            </li>
          </ul>
        </li>
        <li>
          <strong>Relative Bound</strong>:
          <ul>
            <li>
              If \(A\) is invertible and \(\mu\) is an eigenvalue of \(A +
              \delta A\), then: \[ \frac{| \lambda - \mu |}{| \lambda |} \leq
              \kappa_p(V) \| A^{-1} \delta A \|_p \]
            </li>
          </ul>
        </li>
      </ul>
      <p>
        <b>Theorem [Bauer-Fike Theorem]</b> Suppose \( A, E \in \mathbb{C}^{m \times m} \) and \( A = X \Lambda X^{-1} \), where
        \[
        \Lambda =
        \begin{bmatrix}
        \lambda_1 & & \\
        & \ddots & \\
        & & \lambda_m
        \end{bmatrix}.
        \]
        Then for any eigenvalue \( \tilde{\lambda} \) of \( \tilde{A} = A + E \), there exists \( \lambda_i \), an eigenvalue of \( A \), such that
        \[
        |\tilde{\lambda} - \lambda_i| \leq \kappa_2(X) \|E\|_2,
        \]
        where \( \kappa_2(X) = \|X\|_2 \|X^{-1}\|_2 \).
      </p>
      <p>
        <b>Proof.</b> Suppose \( 0 \neq x \in \mathbb{C}^m \) is an eigenvector of \( \tilde{A} \) corresponding to \( \tilde{\lambda} \). Then for \( y = X^{-1} x \),
        \[
        (A + E - \tilde{\lambda} I)x = 0 \implies (X \Lambda X^{-1} - \tilde{\lambda} I + E)x = 0 \implies (\Lambda - \tilde{\lambda} I + X^{-1} EX)y = 0,
        \]
        and from which
        \[
        (\Lambda - \tilde{\lambda} I)y = -X^{-1} EX y.
        \]
        Then
        \[
        \sigma_{\min}(\Lambda - \tilde{\lambda} I)\|y\|_2 \leq \|(\Lambda - \tilde{\lambda} I)y\|_2 = \|X^{-1} EX y\|_2 \leq \|X^{-1} EX\|_2 \|y\|_2.
        \]
        Since \( \Lambda - \tilde{\lambda} I \) is diagonal,
        \[
        \sigma_{\min}(\Lambda - \tilde{\lambda} I) = \min_j |\lambda_j - \tilde{\lambda}|.
        \]
        Suppose the minimum is attained when \( j = i \). Then
        \[
        |\lambda_i - \tilde{\lambda}| \leq \|X^{-1} EX\|_2 \leq \kappa_2(X) \|E\|_2.
        \]
      </p>
      <p>
        <i><b>"Lecture note No. 22"</b></i>
      </p>
    </section>

    <br>

    <section id = "positive_definite">
      <h2>Positive Definite</h2>
      <p>
        The following definitions all involve the term \( x^* M x \). Notice that this is always a real number for any Hermitian square matrix \( M \).

        An \( n \times n \) Hermitian complex matrix \( M \) is said to be <i><b>positive-definite</b></i> if \( x^* M x > 0 \) for all non-zero \( x \) in \( \mathbb{C}^n \). Formally,
        \[ M \text{positive-definite} \iff x^* M x > 0 \text{ for all } x \in \mathbb{C}^n \setminus \{0\} \]
        An \( n \times n \) Hermitian complex matrix \( M \) is said to be <i><b>positive semi-definite</b></i> or <i><b>non-negative-definite</b></i> if \( x^* M x \geq 0 \) for all \( x \) in \( \mathbb{C}^n \). Formally,
        \[ M \text{positive semi-definite} \iff x^* M x \geq 0 \text{ for all } x \in \mathbb{C}^n \]
      </p>
      <p>
        <b>Theorem.</b> A matrix \( M \) is positive-definite if and only if it satisfies any of the following equivalent conditions:
      </p>
      <ul>
        <li>\( M \) is congruent with a diagonal matrix with positive real entries.</li>
        <li>\( M \) is symmetric or Hermitian, and all its eigenvalues are real and positive.</li>
        <li>\( M \) is symmetric or Hermitian, and all its leading principal minors are positive.</li>
        <li>There exists an invertible matrix \( B \) with conjugate transpose \( B^* \) such that \( M = B^* B \).</li>
      </ul>
    </section>

    <br>

    <section id = "gerschgorin_theorem">
      <h2>Gerschgorin’s Theorem</h2>
      <p>
        <b>Theorem [Row version Gerschgorin’s Theorem]</b> Let \( A \in \mathbb{C}^{m \times m} \). Consider \( m \) disks in the complex plane:
        \[ D_i = \left\{ z \mid |z - a_{ii}| \leq \sum_{j \neq i} |a_{ij}| = |a_{i1}| + \ldots + |a_{i,i-1}| + |a_{i,i+1}| + \ldots + |a_{im}| \right\}. \]
        Then we have the following results.
      </p>
      <ul>
        <li>
          Any eigenvalue \( \lambda \in \Lambda(A) \) must lie in at least one of these disks.
        </li>
        <li>
          Suppose the \( m \) disks form \( n \) disjoint regions \( \mathcal{R}_1, \ldots, \mathcal{R}_n \), and each \( \mathcal{R}_k \) (\( 1 \leq k \leq n \)) is connected and consists of \( p_k \) disks. Then for each \( k \) from 1 to \( n \), exactly \( p_k \) eigenvalues of \( A \) are in \( \mathcal{R}_k \).
        </li>
      </ul>
      <br>
      <p>
        <b>Theorem [Column version Gerschgorin’s Theorem]</b> Let \( A \in \mathbb{C}^{m \times m} \). Consider \( m \) disks in the complex plane:
        \[
        \tilde{D}_i = \left\{ z \mid |z - a_{ii}| \leq \sum_{j \neq i} |a_{ji}| = |a_{1i}| + \ldots + |a_{i-1,i}| + |a_{i+1,i}| + \ldots + |a_{mi}| \right\}.
        \]
        Then we have the following results.
      </p>
        <ul>
            <li>
                Any eigenvalue \( \lambda \in \Lambda(A) \) must lie in at least one of the \( m \) disks.
            </li>
            <li>
                The \( m \) disks form \( r \) disjoint regions \( \tilde{\mathcal{R}}_1, \ldots, \tilde{\mathcal{R}}_r \). Suppose each \( \tilde{\mathcal{R}}_k \) (\( 1 \leq k \leq r \)) is connected and consists of \( q_k \) disks. Then for each \( k \) from 1 to \( r \), exactly \( q_k \) eigenvalues of \( A \) are in \( \tilde{\mathcal{R}}_k \).
            </li>
        </ul>
      <p>
        <b>Theorem [Row version Generalized Gerschgorin’s Theorem]</b> Let \( A \in \mathbb{C}^{m \times m} \). Consider \( m \) disks in the complex plane:
        \[
        D_i = \left\{ z \mid |z - a_{ii}| \leq \frac{1}{|d_i|} \sum_{j \neq i} |a_{ij}||d_j| \right\},
        \]
        where \( d_1, \ldots, d_n \) are arbitrary nonzero numbers. Then we have the following results.
      </p>
        <ul>
            <li>
                Any eigenvalue \( \lambda \in \Lambda(A) \) must lie in at least one of these disks.
            </li>
            <li>
                Suppose the \( m \) disks form \( n \) disjoint regions \( \mathcal{R}_1, \ldots, \mathcal{R}_n \), and each \( \mathcal{R}_k \) (\( 1 \leq k \leq n \)) is connected and consists of \( p_k \) disks. Then for each \( k \) from 1 to \( n \), exactly \( p_k \) eigenvalues of \( A \) are in \( \mathcal{R}_k \).
            </li>
        </ul>
      <p>
        <i><b>"Lecture note No. 22"</b></i>
      </p>
    </section>
    <br>

    <section id = "rayleigh_quotient">
      <h2>Rayleigh Quotient</h2>
      <p>
        <b>Definition.</b> Let \( A \in \mathbb{C}^{m \times m} \). For a given vector \( 0 \neq x \in \mathbb{C}^m \), the Rayleigh quotient of \( x \) is the scalar defined by
        \[ r(x) = \frac{x^* A x}{x^* x} \]
      </p>
      <p>
        <b>Algorithm: Rayleigh quotient iteration</b>
      </p>
      <p>
        Choose an initial vector \( x^{(0)} \in \mathbb{C}^m \) with \( \|x^{(0)}\|_2 = 1 \).
      </p>
      <p>
        Compute \( \mu^{(0)} = (x^{(0)})^* A x^{(0)} \).
      </p>
      <p>
        For \( k = 1, 2, \ldots \)
      </p>
      <ul>
        <li>Solve \( (A - \mu^{(k-1)} I) y^{(k)} = x^{(k-1)} \) for \( y^{(k)} \)</li>
        <li>Compute \( \nu_k = \|y^{(k)}\|_2 \)</li>
        <li>Compute \( x^{(k)} = y^{(k)} / \nu_k \)</li>
        <li>Compute \( \mu^{(k)} = (x^{(k)})^* A x^{(k)} \)</li>
      </ul>
      <p>End</p>
    </section>

    <br>

    <section id = 'upper_hessenberg_matrix'>
        <h2>Upper Hessenberg Matrix</h2>
      <p>
        <b>Definition.</b> A matrix \( H = [h_{ij}] \in \mathbb{C}^{m \times m} \) is upper Hessenberg if \( h_{ij} = 0 \) for all \( i \geq j + 2 \), or equivalently,
        \[
        H =
        \begin{bmatrix}
        h_{11} & h_{12} & \cdots & h_{1m} \\
        h_{21} & h_{22} & \cdots & h_{2m} \\
        \vdots & \vdots & \ddots & \vdots \\
        h_{m,m-1} & h_{mm}
        \end{bmatrix}
        \]
        \( H \) is called unreduced upper Hessenberg if \( h_{21}, \ldots, h_{m,m-1} \neq 0 \). Otherwise \( H \) is reduced upper Hessenberg.
      </p>
    </section>

    <br>

    <h2>Qualify Exam Problems (August 2017)</h2>
    <section id = "august_2017">
      <p>
        <b>Problem 2.</b>
      </P>
      <ul>
        <li>
          (a) Find the coefficients and nodes of a Gaussian quadrature formula of the form
          \[
          \int_a^b f(x) \, dx \approx A_0 f(x_0) + A_1 f(x_1). \tag{1}
          \]
        </li>
        <li>
          (b) What is the degree of (algebraic) precision of the formula (1) with the found coefficients and nodes?
        </li>
        <li>
          (c) Prove that no Gaussian quadrature formula of the form (1) can be exact for all polynomials of degree \( \leq 4 \).
        </li>
      </ul>
      <p>
        <b>Solution (a).</b>
<!--        Firstly, we try to find the coefficients and nodes of the Gaussian quadrature formula of the form-->
<!--        \[-->
<!--        \int_{-1}^{1} f(x) \, dx \approx A_0 f(x_0) + A_1 f(x_1).-->
<!--        \]-->
<!--        Set \(f(x) = 1\), we have-->
<!--        \[-->
<!--        \begin{align}-->
<!--        \int_{-1}^{1} 1 \, dx = A_0 + A_1 \\-->
<!--        2 = A_0 + A_1.-->
<!--        \end{align}-->
<!--        \]-->
<!--        Set \(f(x) = x\), we have-->
<!--        \[-->
<!--        \begin{align}-->
<!--        \int_{-1}^{1} x \, dx = A_0 x_0 + A_1 x_1 \\-->
<!--        0 = A_0 x_0 + A_1 x_1 \tag*{(2)}-->
<!--        \end{align}-->
<!--        \]-->
<!--        Set \(f(x) = x^2\), we have-->
<!--        \[-->
<!--        \begin{align}-->
<!--        \int_{-1}^{1} x^2 \, dx = A_0 x_0^2 + A_1 x_1^2 \\-->
<!--        \frac{2}{3} = A_0 x_0^2 + A_1 x_1^2 \tag*{(3)}-->
<!--        \end{align}-->
<!--        \]-->
<!--        Since \(A_0\) and \(A_1\) have the same weight, we can set \(A_0 = A_1\). Thus, we have \(A_0 = A_1 = 1\).-->
<!--        From (2), we have \(x_0 + x_1 = 0\), which implies that \(x_0 = -x_1\).-->
<!--        From (3), we have \(x_0^2 + x_1^2 = 2x_1^2 = \frac23 \), which implies that \(x_1 = \frac{\sqrt{3}}{3}\) and \(x_0 = -\frac{\sqrt{3}}{3}\).-->
<!--        Thus, we can see that-->
<!--        \[-->
<!--        \int_{-1}^{1} f(x) \, dx \approx f\left(-\frac{\sqrt{3}}{3}\right) + f\left(\frac{\sqrt{3}}{3}\right).-->
<!--        \]-->
        Firstly, we set \(f(x) = 1\), we have
        \[
          \int_a^b f(x) \, dx = b - a =  A_0 + A_1.
        \]
        Given that \(A_0\) and \(A_1\) are the same, we have \(A_0 = A_1 = \frac{b - a}{2}\).
        Now, set \(f(x) = x\), we have
        \[
          \int_a^b x \, dx = \frac{b^2}{2} - \frac{a^2}{2} = \frac{(b-a)\cdot (b + a)}{2}= \frac{b - a}{2} x_0 + \frac{b - a}{2} x_1.
        \]
        Thus, we get \(x_0 + x_1 = b+a\), which implies that \(x_0 = b + a - x_1\).
        Then, we set \(f(x) = x^2\), we have
        \[
          \int_a^b x^2 \, dx = \frac{b^3}{3} - \frac{a^3}{3} = \frac{(b-a)\cdot (b^2 + ab + a^2)}{3} = \frac{b - a}{2} x_0^2 + \frac{b - a}{2} x_1^2.
        \]
        It shows that
        \[
        \frac{2\cdot (b^2 + ab + a^2)}{3} = x_0^2 + x_1^2.
        \]
        We substitute \(x_0 = b + a - x_1\) into the equation above, we get
        \[
        \begin{align}
        \frac{2\cdot (b^2 + ab + a^2)}{3} &= (b + a - x_1)^2 + x_1^2 \\
        \frac{2\cdot ((a + b)^2 - ab)}{3}&= (b + a)^2 - 2x_1(b+a) + 2x_1^2 \\
        2x_1^2 - 2x_1(b+a) + (b + a)^2  &- \frac{2\cdot ((a + b)^2 - ab)}{3} = 0 \\
        2x_1^2 - 2(b+a)x_1 + \frac{(a + b)^2}{3} + \frac{2ab}{3} &= 0 \\
        \end{align}
        \]
        Now, using the quadratic formula, we have
        \[
        x_1 = \frac{2(b+a) \pm \sqrt{4(b+a)^2 - 4\cdot 2\cdot \left(\frac{(a + b)^2}{3} + \frac{2ab}{3}\right)}}{4} = \frac{b+a \pm \sqrt{\frac13b^2 - \frac23ab + \frac13a^2}}{2} = \frac{b+a \pm \frac{\sqrt{3}}{3}(b-a)}{2}.
        \]
        Hence, we have \(x_0 = \frac{b+a}{2} - \frac{\sqrt{3}(b-a)}{6}\), and \(x_1 = \frac{b+a}{2} + \frac{\sqrt{3}(b-a)}{6}\).
        Therefore, we have
        \[
        \int_a^b f(x) \, dx \approx \frac{b - a}{2} f\left(\frac{b+a}{2} - \frac{\sqrt{3}(b-a)}{6}\right) + \frac{b - a}{2} f\left(\frac{b+a}{2} + \frac{\sqrt{3}(b-a)}{6}\right).
        \]
      </p>
    </section>

    <br>

    <h2>Qualify Exam Problems (January 2019)</h2>
    <section id = "january_2019">
      <ul>
          <li>
          Derive the two-step Adams-Moulton method
          \[ x_{k+2} = x_{k+1} + \frac{h}{12} \left( 5f(t_{k+2}, x_{k+2}) + 8f(t_{k+1}, x_{k+1}) - f(t_k, x_k) \right), \]
          </li>
          <li>
          Study the consistency, zero-stability, absolute stability, and convergence of this method for the initial value problem \( x' = f(t, x(t)) \).
          </li>
      </ul>
      <p>
      <b>Proof.</b>

      </p>

      <br>

      <p>
        <b>Problem 4.</b> The Lagrange interpolating polynomial \( p_n \in P_n \) that interpolates function \( f \in C([a, b]) \) at distinct points \( x_i \in [a, b], \, i = 0, \ldots, n \) is given as
        \[ p_n(x) = \sum_{i=0}^{n} f(x_i) \ell_i(x), \]
        where
        \[ \ell_i = \prod_{\substack{j=0 \\ j \neq i}}^{n} \frac{(x - x_j)}{(x_i - x_j)}. \]
        Let \( w(x) = \prod_{j=0}^{n} (x - x_j) \) and \( \beta_i = \frac{1}{\prod_{\substack{j=0 \\ j \neq i}}^{n} (x_i - x_j)}, \, i = 0, \ldots, n. \)
      </p>
      <ul>
        <li>
          (a) Show that evaluating \( p_n(x) \) directly from (1) requires \( \mathcal{O}(n^2) \) floating-point operations.
        </li>
        <li>
          (b) Show that the Lagrange interpolating polynomial \( p_n(x) \) can be rewritten as
          \[ p_n(x) = w(x) \sum_{i=0}^{n} \frac{\beta_i}{x - x_i} f(x_i). \]
        </li>
        <li>
          (c) Show that
          \[ 1 = w(x) \sum_{i=0}^{n} \frac{\beta_i}{x - x_i}\text{ and }p_n(x) = \frac{\sum_{i=0}^{n} \frac{\beta_i}{x - x_i} f(x_i)}{\sum_{i=0}^{n} \frac{\beta_i}{x - x_i}}. \]
          Note: values \( \beta_i \) are called the barycentric weights, and this alternative Lagrange interpolation formula is known as the barycentric interpolation formula.
        </li>
      </ul>
      <p>
        <b>Proof (b).</b>
        Given that \(\beta_i = \frac{1}{\prod_{\substack{j=0 \\ j \neq i}}^{n} (x_i - x_j)}\), and \(w(x) = \prod_{j=0}^{n} (x - x_j)\), we have
        \[
          w(x)\cdot \frac{\beta_i}{x - x_i} = \frac{\prod_{j=0}^{n} (x - x_j)}{\prod_{\substack{j=0 \\ j \neq i}}^{n} (x_i - x_j)} \cdot \frac{1}{x - x_i} = \frac{\prod_{\substack{j=0 \\ j \neq i}}^{n} (x - x_j)}{\prod_{\substack{j=0 \\ j \neq i}}^{n} (x_i - x_j)} = \ell_i(x).
        \]
        Therefore, we can rewrite the Lagrange interpolating polynomial \( p_n(x) \) as
        \[
          p_n(x) = \sum_{i=0}^{n} f(x_i) \ell_i(x) = \sum_{i=0}^{n} f(x_i) w(x) \frac{\beta_i}{x - x_i} = w(x) \sum_{i=0}^{n} \frac{\beta_i}{x - x_i} f(x_i). \tag*{\(\square\)}
        \]
      </p>
      <p>
        <b>Proof (c).</b>
        Since we showed that \(\ell_i(x) = w(x) \cdot \frac{\beta_i}{x - x_i}\) above, we have
        \[
          w(x)\sum_{i=0}^{n} \frac{\beta_i}{x - x_i} = \sum_{i=0}^{n} w(x)\frac{\beta_i}{x - x_i} = \sum_{i=0}^{n} \ell_i(x).
        \]
        Suppose that \(f(x) = 1\) for all \(x\). Then, we have the polynomial interpolation \(p(x)\) for \(f(x) = 1\) as
        \[
          p(x) = \sum_{i=0}^{n} f(x_i) \ell_i(x) = \sum_{i=0}^{n} \ell_i(x).
        \]
      </p>
  </section>

  <br>

    <section>
      <p>
        <b>Problem 2.</b>
      </p>
      <p>
        (a) Suppose \( g(x) \in C([a,b]) \) with \( a \leq g(x) \leq b \) for all \( x \in [a,b] \).
      </p>
      <ul>
        <li>
          Show that \( g(x) \) has at least one fixed point in \( [a,b] \).
        </li>
        <li>
          Show that if \( g'(x) \) exists on \( [a,b] \) and \( |g'(x)| \leq k < 1 \) for \( x \in [a,b] \), then this fixed point is unique.
        </li>
      </ul>
      <p>
        (b) Consider the nonlinear equation \( x^2 - 1 = 0 \). Propose two ways of rewriting this equation as a fixed-point problem \( x = f(x) \) using different functions \( f \). Discuss the convergence (or non-convergence) behavior of the fixed-point iteration \( x_{n+1} = f(x_n) \), \( n = 0,1,2, \ldots \), for each of these two iteration functions.
      </p>
      <p>
        <b>Proof (i).</b>Define \(h(x) = x\) where \(x\in [a, b]\). Hence, we can know that \(h(x)\in C([a, b])\).
        Since \(g(x)\in C([a, b])\), we can know that \(f(x) = g(x) - h(x) = g(x) - x\) is in \(C([a, b])\).
        If \(f(a) = a\) or \(f(b) = b\), then we are done.
        Now, suppose that \(f(a)\neq a\) and \(f(b)\neq b\). Since \(a\leq f(x)\leq b\) for all \(x\in [a, b]\), we can know that \(f(a) > a\) and \(f(b) < b\).
        Hence, we have \(f(a) = g(a) - a > 0\) and \(f(b) = g(b) - b < 0\).
        Since \(f(x)\in C([a, b])\), by the Intermediate Value Theorem, for any \(c\in [f(b), f(a)]\), there exists \(x\in [a, b]\) such that \(f(x) = c \).
        Now, set \(c = 0\), we know that there exists \(x\in [a, b]\) such that \(f(x) = 0\). Therefore, we can know that \(g(x) = x\) for some \(x\in [a, b]\). \( \blacksquare \)
      </p>
      <p>
        <b>Proof (ii).</b>Since \(g(x)\in C([a, b])\), according to the Mean Value Theorem, we have
        \[
          g(y) - g(x) = g'(\xi)(y - x) \text{ for some } \xi \in (x,y), \text{ where }a\leq x\leq y\leq b.
        \]
        Thus, we have
        \[
          |g(y) - g(x)| = |g'(\xi)| |y - x| \leq k |b - a|< 1\cdot |y - x|.
        \]
        Hence, we can see that \(g(x)\) is a contraction mapping. Therefore, by the Contraction Mapping Theorem, we have that the fixed point is unique. \( \blacksquare \)
      </p>
    </section>

    <h2 id = 'january_2020'>Qualify Exam Problems (January 2020)</h2>
    <p>
      <b>Problem 4.</b> Let \( A \in \mathbb{C}^{n \times n} \) be a non-singular matrix, and let \( 0 \neq b \in \mathbb{C}^n \) be a fixed vector. Suppose that you have an algorithm for solving \( Ax = b \) that produces an approximate solution \( \tilde{x} (\neq 0) \) that satisfies

      \[
      (A + \Delta A) \tilde{x} = b,
      \]

      for some matrix \( \Delta A \) such that

      \[
      \frac{\|\Delta A\|}{\|A\|} \leq \epsilon,
      \]

      with any consistent matrix norm \( \|\cdot\| \).
    </p>
    <ul>
      <li>
      (a) Prove that for every \( \epsilon > 0 \)

      \[
      \frac{\|\tilde{x} - x\|}{\|\tilde{x}\|} \leq \kappa(A) \epsilon,
      \]

      where \( x \in \mathbb{C}^n \) is the exact solution of \( Ax = b \), and \( \kappa(A) \) is the condition number of \( A \).
        </li>
        <li>
      (b) Suppose that (a) holds asymptotically as \( \epsilon \to 0 \). Prove that

      \[
      \frac{\|\tilde{x} - x\|}{\|x\|} \leq \kappa(A) \epsilon + \mathcal{O}(\epsilon^2), \quad \text{as} \quad \epsilon \to 0.
      \]
        </li>
    </ul>
    <p>
      <b>Proof.</b> Given that \((A + \Delta A) \tilde{x} = b \) and \(A x = b\), we have
      \[
        \begin{align}
      Ax - (A + \Delta A) \tilde{x} &= 0 \\
      Ax - A\tilde{x} - \Delta A \tilde{x} &= 0 \\
      A(x - \tilde{x}) &= \Delta A \tilde{x}.
        \end{align}
        \]
      Since \(A\) is non-singular, we have
        \[
        x - \tilde{x} = A^{-1} \Delta A \tilde{x}.
      \]
      By Triangle Inequality, we have
        \[
        \|x - \tilde{x}\| = \|A^{-1} \Delta A \tilde{x}\| \leq \|A^{-1}\| \|\Delta A\| \|\tilde{x}\|.
        \]
      Since \(A\) is non-singular and \(b\neq 0\), if \(x = 0\) then \(b = 0\) since \(A\cdot x = 0\), which is a contradiction.
      Thus, we can know that \(x\neq 0\), which implies that \(\|x\|>0\). Therefore, we have
        \[
        \frac{\|x - \tilde{x}\|}{\|\tilde{x}\|} \leq \|A^{-1}\| \|\Delta A\|.
        \]
      Since \(\frac{\|\Delta A\|}{\|A\|} \leq \epsilon\), we have
        \[
        \begin{align}
      \|\Delta A\| &\leq \epsilon \|A\| \\
        \frac{\|x - \tilde{x}\|}{\|\tilde{x}\|} \leq \|A^{-1}\| \|\Delta A\| &\leq \|A^{-1}\| \|A\| \epsilon = \kappa(A) \epsilon. \tag*{\(\square\)}
        \end{align}
        \]
    </p>

    <br>

    <p>
      <b>5.</b> Let matrix \( A \in \mathbb{R}^{n \times n} \) be positive definite and matrix \( X \in \mathbb{R}^{n \times k} \) be of full column rank, i.e., \(\text{rank}(X) = k \).
    </p>
      <ul>
        <li>
          (a) Write down a definition of the positive definite matrix.
        </li>
        <li>
          (b) Prove that the matrix \( B = X^TAX, \, B \in \mathbb{R}^{k \times k} \) is positive definite.
        </li>
        <li>
          (c) Prove that all principal sub-matrices of matrix \( A \) are positive definite. In particular, all the diagonal entries are positive.
        </li>
        <li>
          (d) Prove that matrix \( A \) has an LU factorization and the diagonal entries of matrix \( U \) are all positive.
        </li>
    </ul>
    <p>
      <b>Solution (a).</b> We say \(A\) is positive definite if for all \(x\in \mathbb{R}^n\setminus \{0\}\), we have \(x^T A x > 0\).
    </p>
    <p>
      <b>Proof (b).</b> Let \(v\in \mathbb{R}^k\setminus \{0\}\). We denote \(X\) as
      \[
        X = \begin{bmatrix} \vec{x_1} & \vec{x_2} & \cdots & \vec{x_k} \end{bmatrix},
        \]
      where \(x_i\in \mathbb{R}^n\). Since \(X\) is of full column rank, we have \(\vec{x_1}, \vec{x_2}, \ldots, \vec{x_k}\) are linearly independent.
      Now, let \(v = \begin{bmatrix} v_1 & v_2 & \cdots & v_k \end{bmatrix}^T\). Then, we have
      \[
        v^T\cdot X^T = \begin{bmatrix} v_1 & v_2 & \cdots & v_k \end{bmatrix} \begin{bmatrix} \vec{x_1}^T \\ \vec{x_2}^T \\ \vdots \\ \vec{x_k}^T \end{bmatrix} = v_1 \vec{x_1} + v_2 \vec{x_2} + \cdots + v_k \vec{x_k}.
      \]
      Since \(\{ \vec{x_1}, \vec{x_2}, \ldots, \vec{x_k} \}\) are linearly independent, we have
      \[
      a_1\vec{x_1} + a_2 \vec{x_2} + \cdots + a_k \vec{x_k} = 0 \quad \text{if and only if} \quad a_1 = a_2 = \cdots = a_k = 0.
        \]
      And we know that there exists some \(v_i\neq 0\). Therefore, we have
      \[
        v^T\cdot X^T \neq 0,\text{ and } (Xv)^T\neq 0,
      \]
      which implies that \(Xv\neq 0\). Now, we denote \(u = Xv\).
      Since \(A\) is positive definite and \(u\neq 0\), we have
        \[
            v^T\cdot X^TAXv = (Xv)^TA(Xv) = u^TAu > 0. \tag*{\(\square\)}
        \]
    </p>
    <p>
      <b>Proof.</b> According to the \(LU\) factorization, we can know that the diagonal entries of \(L\) are all ones.
      We denote \(L_k, U_k\) as the \(k\)th leading principal sub-matrix of \(L, U\), respectively.
      And we can know that \(A_k = L_kU_k\). Since \(\det A_k>0\) and \(\det L_k = 1\) and \(\det A_k = \det U_k \det L_k\),
      we can know that \(\det U_k > 0\). Since \(\det U_k = u_{11} \dots u_{kk}\), we can know that \(u_{11}\cdots u_{kk} > 0\).
      Now, we want to prove that \(u_{ii}>0\) by induction.
      Since \(\det U_1 > 0\), we can know that \(u_{11} > 0\).
      Suppose that \(u_{11}\cdots u_{ii} > 0\). Then, we have \(\det U_{i+1} = u_{11}\cdots u_{ii}u_{i+1,i+1} > 0\).
      It forces that \(u_{i+1,i+1} > 0\). Therefore, we can know that the diagonal elements of \(U\) are positive.
      Thus, \(D = \text{diag}(\sqrt{u_{11}}, \dots, \sqrt{u_{mm}})\) exists and is non-singular.

      Let \(R = D^{-1}U\). Since \(D^{-1} = \text{diag}(\frac{1}{\sqrt{u_{11}}}, \dots, \frac{1}{\sqrt{u_{mm}}})\), we can know that \(D^{-1}\) is an upper triangular matrix.
      Since \(U\) is an upper triangular matrix, we can know that \(R\), which is a product of two upper triangular matrices, is an upper triangular matrix.

      Now, we want to show that \(A = R^*R\).
      Since \(A\) is Hermitian, we have \(A = A^*\).
      Let \(A = LU\), then \(A = (LU)^* = U^*L^* = LU\).
      Since the diagonal entries of \(L\) are \(U\) are positive, we can know that \(L, U\) are non-singular, which implies that \(U^*, L^*\) are also non-singular.
      Then, given \(U^*L^* = LU\), we can know that \(U(L^*)^{-1} = L^{-1}U^*\).
      Again, according to the properties of upper and lower triangular matrices, we can know that \((L^*)^{-1}\) is an upper triangular matrix and \( U^*\) is a lower triangular matrix.
      Hence, \(U(L^*)^{-1}\) is an upper triangular matrix and \(L^{-1}U^*\) is a lower triangular matrix.
      Since \(U(L^*)^{-1} = L^{-1}U^*\), we can know that \(U(L^*)^{-1} = L^{-1}U^*\) is a diagonal matrix.
      Now, we look at the entries of \(L^{-1}U^*\). Firstly, the diagonal entries of \(L^{-1}\) are still ones and the diagonal entries of \(U^*\) are the same as the diagonal entries of \(U\).
      Now, by the matrix multiplication, we see that the diagonal entries of \(L^{-1}U^*\) are \(u_{11}, \dots, u_{mm}\).
      Since itself is a diagonal matrix, we can know that
      \[
      L^{-1}U^* = \text{diag}(u_{11}, \dots, u_{mm}) = D^2.
      \]
      It shows that \(U^* = LD^2\).
      Given \(D^{-1}\) is a diagonal matrix, we can know that \((D^{-1})^* = D^{-1}\).
      \begin{align*}
      (D^{-1}U)^* &= U^*(D^{-1})^* = U^*D^{-1} = LD^2D^{-1} = LD.
      \end{align*}
      Therefore, we can see that
      \[
      R^* R = (D^{-1}U)^*(D^{-1}U) = LDD^{-1}U = LU = A. \tag*{\(\square\)}
      \]
    </p>

    <br>

    <h2 id = 'august_2020'>Qualify Exam Problems (August 2020)</h2>
    <section>
      <p>
        <b>1</b>. Consider the function \( g(x) = \frac{1}{2}(1 + 2x - x^2) \) defined on the closed interval \([1/2, 3/2]\).
      </p>
      <ul>
        <li>
          (a) Prove that \( g(x) \) has a unique fixed point on the given interval. Also, determine this fixed point.
        </li>
        <li>
          (b) Prove that for any initial point \( x_0 \in [1/2, 3/2] \), the number sequence \( \{x_n\} \) generated by the fixed point iteration \( x_n = g(x_{n-1}) \) converges to the unique fixed point.
        </li>
        <li>
          (c) Determine the convergence rate of the fixed point iteration described in 1(b).
        </li>
      </ul>
      <p>
        <b>Proof (a).</b> Suppose that \(a, b\in [1/2, 3/2]\) and \(a\neq b\). Then, we have
        \[
        |g(a) - g(b)| = \left|\frac{1}{2}(1 + 2a - a^2) - \frac{1}{2}(1 + 2b - b^2)\right| = \left|\frac12 (b^2 - a^2) + a - b\right| = \left|\frac12(b-a)(b+a) + a - b\right|.
        \]
        \[
        \left|\frac{g(a) - g(b)}{a - b}\right| = \left|-\frac12(b+a) + 1\right|.
        \]
        Since \(a, b\in [1/2, 3/2]\), we have \(1 \leq b+a \leq 3\). Then, we can have \(|-\frac12(b+a) + 1| \leq \frac12\).
        Thus, we can know that
        \[
        |g(a) - g(b)| \leq \frac12 |a - b|.
        \]
        By the Contraction Mapping Theorem, we can know that \(g(x)\) has a unique fixed point on the given interval. \( \blacksquare \)
      </p>
      <p>
        <b>Proof (b).</b> Since we already showed that \(g(x)\) is a contraction mapping, we can know that the fixed point iteration \(x_n = g(x_{n-1})\) converges to the unique fixed point by contractive mapping theorem. \( \blacksquare \)
      </p>
      <p>
        <b>Solution (c).</b> We firstly found the point it will converge to. Let \(g(x) = x\).
        We have
        \[
        \frac{1}{2}(1 + 2x - x^2) = x.
        \]
        We solve for \(x\) and we have \(x = \pm 1\). Since it is in \([1/2, 3/2]\), we have \(x = 1\).
        Now, we let
        \[
        \dfrac{\left|\frac12(1 + 2x - x^2) - 1\right|}{\left|x - 1\right|} = \dfrac{\left|\frac12(1 + 2x - x^2) - 1\right|}{\left|x - 1\right|} = \dfrac{\left|\frac12(1 + 2x - x^2) - 1\right|}{\left|x - 1\right|} = \dfrac{\left|-\frac12(x-1)^2\right|}{\left|x - 1\right|} = \dfrac{\frac12|x-1|^2}{|x-1|} = \dfrac12|x-1|.
        \]
        Thus, we can see that
        \[
        \dfrac{\left|\frac12(1 + 2x - x^2) - 1\right|}{\left|x - 1\right|^2} = \dfrac12.
        \]
        Therefore, we say that the convergence rate of the fixed point iteration is quadratic.
      </p>
    </section>

    <br>

    <section>
      <p>
        <b>2.</b> (a) Consider the Gaussian quadrature of the form

        \[
        G_2(f) = w f(-d) + w f(d) \approx \int_{-h}^{h} f(x) \, dx, \tag*{(1)}
        \]

        where \( h \) is a given positive number and \( w, d \) are unknown real numbers. Determine this Gaussian quadrature by finding \( w \) and \( d \).
      </p>
      <p>
        (b) The Gaussian quadrature (1) has an error formula

        \[
        \int_{-h}^{h} f(x) \, dx - G_2(f) = C f^{(r+1)}(\xi),
        \]

        where \( \xi \in [-h, h] \). Determine the constant \( C \) and the integer \( r \).
      </p>
      <p>
        (c) Consider the composite Gaussian quadrature \( G_m(f) \) that approximates the integral

        \[
        I(f) = \int_{0}^{b} f(x) \, dx.
        \]

        Let \( m > 0 \) be an integer and \( h = b/(2m) \). Divide the interval \([0, b]\) into \( m \) equally spaced sub-intervals \([x_k, x_{k+1}]\) of length \( 2h \) for \( k = 0, 1, \ldots, m-1 \), where the endpoints are determined by \( x_k = 2hk \) for \( k = 0, 1, \ldots, m \). The composite Gaussian quadrature \( G_m(f) \) is constructed by adapting (1) to each sub-interval \([x_k, x_{k+1}]\) ( \( k = 0, \ldots, m-1 \)). Derive a formula for \( G_m(f) \) and the error formula

        \[
        I(f) - G_m(f) = D f^{(s+1)}(\eta), \quad \eta \in [0, b].
        \]

        The constant \( D \) and the integer \( s \) need to be provided.
      </p>
      <p>
        <b>Solution (a).</b> Let \(f(x) = 1\), we have
        \[
          \int_{-h}^{h} f(x) \, dx = \int_{-h}^{h} 1 \, dx = 2h = w\cdot 1 + w\cdot 1 = 2w.
        \]
        Hence, we have \(w = h\). Now, set \(f(x) = x\), we have
        \[
          \int_{-h}^{h} f(x) \, dx = \int_{-h}^{h} x \, dx = 0 = w\cdot (-d) + w\cdot d = 0,
        \]
        which does not provide too much information. Now, we set \(f(x) = x^2\), we have
        \[
          \int_{-h}^{h} f(x) \, dx = \int_{-h}^{h} x^2 \, dx = \frac{2h^3}{3} = h\cdot d^2 + h\cdot d^2 = 2hd^2.
        \]
        Thus, we have \(d = \frac{\sqrt{3}h}{3}\). Hence, we have \(w = h\) and \(d = \frac{\sqrt{3}h}{3}\).
      </p>
    </section>

    <section>
      <p>
        <b>3.</b> Consider the numerical ODE method
        \[
        y_{k+1} = y_{k-1} + \frac{h}{3} \left( f(t_{k+1}, y_{k+1}) + 4f(t_k, y_k) + f(t_{k-1}, y_{k-1}) \right)
        \]
        for solving the initial value problem \( y' = f(t, y) \) for \( t \in (a, b] \) with \( y(a) = \alpha \), where \( h = \frac{(b-a)}{n} \), \( t_k = a + kh \) for \( k = 0, 1, \ldots, n \), and \( n > 0 \) is an integer.
        </p>
        <p>
        (a) State whether this method is explicit or implicit, single-step or multistep.
        </p>
        <p>
        (b) State and justify whether this method is weakly stable, strongly stable, or unstable.
        </p>
        <p>
        (c) Derive the order of the local truncation error.
      </p>
      <p>
        <b>Solution (a).</b> This method is implicit and multistep.
      </p>
      <p>
        <b>Solution (b).</b> We firstly find the characteristic polynomial of the method.
        Hence, we have \(p(x) = x^2 - x\). Then, we can know that \(p(x) = x(x-1)\). Since the roots of the characteristic polynomial are \(0\) and \(1\), \(|0| < 1\) and \(p(1) = 0\).
        Moreover, \(p'(x) = 2x - 1\), and \(p'(1) = 1 \neq 0\). Therefore, we can know that this method is strongly stable.
      </p>
    </section>

    <section>
      <h2>Qualify Exam Problems (January 2022)</h2>
      <p>
        1. Consider the iteration \[ x_{n+1} = \frac{1}{2} x_n + \frac{5}{2x_n},
        \quad n = 0, 1, \ldots \]
      </p>
      <ul>
        <li>
          Show that if \(x_0 > 0\), then \(x_n \geq \sqrt{5}\) for \(n =
          1,2,\ldots\).
        </li>
        <li>
          se the Contractive Mapping Theorem to show that the iteration is
          convergent.
        </li>
        <li>
          Show that the order of convergence of the iteration is quadratic.
        </li>
      </ul>
      <h2>Solutions</h2>
      <p>
        \(\textbf{Proof of 1.}\) Let \(f(x) = \frac{1}{2}x + \frac{5}{2x}\).
        Hence, we can calculate the derivative of \(f(x)\) as \[ f'(x) =
        \frac{1}{2} - \frac{5}{2x^2} = \frac{1}{2}\left(1 - \frac{5}{x^2}\right)
        \] Since \(x_0 > 0\), we just assume that \(x>0\). When \( 0 \lt x \leq
        \sqrt{5} \), we have \( f'(x) \lt 0 \). When \( x \geq \sqrt{5} \), we
        have \( f'(x) \geq 0 \). Thus, we can know that when \( x = \sqrt{5} \),
        \( f(x) \) is at minimum for \( f(x) \) where \( x \geq 0 \). Then, \[
        \min f(x) = f(\sqrt{5}) = \frac{1}{2} \sqrt{5} + \frac{5}{2\sqrt{5}} =
        \frac{1}{2} \sqrt{5} + \frac{1}{2} \sqrt{5} = \sqrt{5}. \] Therefore, we
        have \( f(x) \geq \sqrt{5} \). In other words, if \( x_0 > 0 \), then \(
        x_n \geq \sqrt{5} \). \[ \tag*{$\square$} \]
      </p>
    </section>

    <br>

    <h2>Qualify Exam Problems (August 2022)</h2>
    <section id="ku_2022_8_1">
      <p>
        \(\textbf{Problem 1. }\)Consider the interpolation problem: Find a
        polynomial of degree less than or equal to three interpolating the table
        \[ \begin{array}{c|cccc} x & 0 & 1 & 7 & 2 \\ \hline f(x) & 51 & 3 & 201
        & 1 \\ \end{array} \]
      </p>
      <ul>
        <li>
          Prove that the solution to the interpolation problem exists and is
          unique.
        </li>
        <li>Find the interpolation polynomial in Newton's form.</li>
        <li>
          Derive the error for the interpolation polynomial provided that
          function \( f = f(x) \) is sufficiently smooth.
        </li>
      </ul>
      <p>
        \(\textbf{Proof. }\)Firstly, we can observe that for all \(x\), they are
        distinct. Therefore, for arbitrary value of \(f(x)\), we can find a
        unique polynomial of degree less than or equal to \(3\) interpolating
        the table. \( \blacksquare \)
      </p>
      <p>
        \(\textbf{Solution.}\)Here we can use Newton's divided difference method to find the interpolation polynomial.
        \[
        \begin{align*}
        x_0 = 0: & f[x_0] = 51, & f[x_0, x_1] = \frac{3 - 51}{1 - 0} = -48, & f[x_0, x_1, x_2] = \frac{33 - (-48)}{7 - 0} = \frac{81}{7}, & f[x_0, x_1, x_2, x_3]
        = \frac{7 - 81/7}{2-0} = -\frac{16}{7},\\ x_1 = 1: & f[x_1] = 3, &
        f[x_1, x_2] = \frac{201 - 3}{7 - 1} = 33, & f[x_1, x_2, x_3] = \frac{40
        - 33}{1-0} = 7, & \\ x_2 = 7: & f[x_2] = 201, & f[x_2, x_3] = \frac{1 -
        201}{2 - 7} = 40, & & \\ x_3 = 2: & f[x_3] = 1, & & & \\ \end{align*} \]
        Therefore, we have \[ p(x) = 51 + (-48)(x-0) + \frac{81}{7}(x-0)(x-1) -
        \frac{16}{7}(x-0)(x-1)(x-7). \]
      </p>
      <p>\(\textbf{Solution.}\)</p>
    </section>

    <section>
      <p>
        \(\textbf{Problem. }\)Let \( A \in \mathbb{C}^{m \times m} \), and \( A
        = U\Sigma V^* \) being a singular value decomposition of \( A \), where
        \( U, V \in \mathbb{C}^{m \times m} \) are unitary and \( \Sigma \) is
        non-negative diagonal. Define \[ W = \frac{1}{\sqrt{2}} \begin{bmatrix}
        U & U \\ V & -V \end{bmatrix} \in \mathbb{C}^{2m \times 2m}. \]
      </p>
      <ul>
        <li>Prove \( W \) is unitary.</li>
        <li>
          Let \[ B = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix} \in
          \mathbb{C}^{2m \times 2m}. \] Prove \( B \) is Hermitian and have the
          spectral decomposition \[ B = W \begin{bmatrix} \Sigma & 0 \\ 0 &
          -\Sigma \end{bmatrix} W^*. \]
        </li>
        <li>Prove \( \|B\|_2 = \|A\|_2 \).</li>
      </ul>
      <p>
        \(\textbf{Proof(a). }\) Given \(W\), we have with Unitary Matrices \(U,
        V\), we can get \[ W^* = \frac{1}{\sqrt{2}} \begin{bmatrix} U^* & V^* \\
        U^* & -V^* \end{bmatrix} \in \mathbb{C}^{2m \times 2m}. \] Then we have
        \begin{align} W^* W &= \frac{1}{2} \begin{bmatrix} U^*U + V^*V & U^*U -
        V^*V \\ U^*U - V^*V & U^*U + V^*V \end{bmatrix}\\ &= \frac{1}{2}
        \begin{bmatrix} I_m + I_m & I_m - I_m \\ I_m - I_m & I_m + I_m
        \end{bmatrix}\\ &= \frac{1}{2} \begin{bmatrix} 2I_m & 0_m \\ 0_m & 2I_m
        \end{bmatrix}\\ &= \begin{bmatrix} I_m & 0_m \\ 0_m & I_m \end{bmatrix}
        \\ &= I_{2m}. \end{align} By the similar process, we have \begin{align}
        WW^* &= \frac{1}{2} \begin{bmatrix} UU^* + UU^* & UV^* - UV^* \\ VU^* -
        VU^* & VV^* + VV^* \end{bmatrix}\\ &= \frac{1}{2} \begin{bmatrix} I_m +
        I_m & 0_m \\ 0_m & I_m + I_m \end{bmatrix}\\ &= I_{2m}. \end{align}
        Therefore, we show that \(W\) is unitary. \[ \tag*{$\square$} \]
      </p>
      <p>
        \(\textbf{Proof(b). }\)Firstly, we have \[ B^* = \begin{bmatrix} 0 & A^*
        \\ (A^*)^* & 0 \end{bmatrix} = \begin{bmatrix} 0 & A^* \\ A & 0
        \end{bmatrix} = B. \] Thus, we have \(B\) is Hermitian. To show the
        spectral decomposition, we need to do calculation. \begin{align} W
        \begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma \end{bmatrix} W^* &=
        \frac{1}{2} \begin{bmatrix} U & U \\ V & -V \end{bmatrix}
        \begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma \end{bmatrix} \begin{bmatrix}
        U^* & V^* \\ U^* & -V^* \end{bmatrix}\\ &= \frac{1}{2} \begin{bmatrix}
        U\Sigma & -U\Sigma \\ V\Sigma & V\Sigma \end{bmatrix} \begin{bmatrix}
        U^* & V^* \\ U^* & -V^* \end{bmatrix}\\ &= \frac{1}{2} \begin{bmatrix}
        U\Sigma U^* - U\Sigma U^* & U\Sigma V^* + U\Sigma V^* \\ V\Sigma U^* +
        V\Sigma U^* & V\Sigma V^* - V\Sigma V^* \end{bmatrix}\\ &=
        \begin{bmatrix} 0 & U\Sigma V^* \\ V\Sigma U^* & 0 \end{bmatrix}\\
        \end{align} Since \(A = U\Sigma V^*\), which is SVD of \(A\), we know
        all entries of \(\Sigma\) are non-negative. Thus, we have \(\Sigma =
        \Sigma^*\). In that case, we have \[ W \begin{bmatrix} \Sigma & 0 \\ 0 &
        -\Sigma \end{bmatrix} W^* = \begin{bmatrix} 0 & U\Sigma V^* \\ V\Sigma
        U^* & 0 \end{bmatrix} = \begin{bmatrix} 0 & U\Sigma V^* \\ V\Sigma^* U^*
        & 0 \end{bmatrix} = \begin{bmatrix} 0 & A \\ A^* & 0 \end{bmatrix} = B.
        \] \[ \tag*{$\square$} \]
      </p>
      <p>
        \(\textbf{Proof(c).}\) Since we just show that \(W\) is unitary, and we
        are given that \(U\) and \(V\) are unitary, we have \begin{align}
        \|A\|_2 &= \|U\Sigma V^*\|_2 = \|\Sigma\|_2 = \max\sigma_i \\ \|B\|_2 &=
        \left\|W \begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma \end{bmatrix}
        W^*\right\|_2 = \left\|\begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma
        \end{bmatrix}\right\|_2. \end{align} Since \(\begin{bmatrix} I_m & 0 \\
        0 & -I_m \end{bmatrix}\) is a unitary matrix, and \[ \begin{bmatrix} I_m
        & 0 \\ 0 & -I_m \end{bmatrix} \begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma
        \end{bmatrix} = \begin{bmatrix} \Sigma_m & 0 \\ 0 & \Sigma_m
        \end{bmatrix}, \] we have \[ \left\|\begin{bmatrix} \Sigma & 0 \\ 0 &
        -\Sigma \end{bmatrix}\right\|_2 = \left\|\begin{bmatrix} I_m & 0 \\ 0 &
        -I_m \end{bmatrix} \begin{bmatrix} \Sigma & 0 \\ 0 & -\Sigma
        \end{bmatrix}\right\|_2 = \left\|\begin{bmatrix} \Sigma_m & 0 \\ 0 &
        \Sigma_m \end{bmatrix}\right\|_2 = \max\sigma_i. \] Therefore, we have
        \(\|B\|_2 = \|A\|_2\). \[ \tag*{$\square$} \] For the second part, we
        prove it by contradiction. Suppose that it is not true. Then, we know
        that there is no singular value \(|\tilde \sigma - \sigma|\leq
        \|E\|_2\). Hence, we can show that \(|\tilde \sigma - \sigma| >
        \|E\|_2\) for all singular values of \(A\). If \(\tilde \sigma >
        \sigma\), then we have \[ \tilde \sigma - \sigma > \|E\|_2. \] Let
        \(\sigma = \|A\|_2\). Then, we have \[ \begin{align} \tilde \sigma -
        \|A\|_2 &> \|E\|_2 \\ \tilde \sigma &> \|A\|_2 + \|E\|_2. \end{align} \]
        Hence, we have \(\|\tilde A\|_2 > \tilde \sigma > \|A\|_2 + \|E\|_2\),
        which is a contradiction. If \(\tilde \sigma \lt \sigma\), then we have
        \[ \sigma - \tilde \sigma > \|E\|_2. \]
      </p>
    </section>

    <br>

    <section>
      <h2>Qualify Exam Problems (January 2023)</h2>
      <p>
        <b>Problem 4.</b> For \( A \in \mathbb{R}^{n \times n} \) and \( A = A^T \), Rayleigh Quotient Iteration is defined by:
      </p>
      <p>
        select \( \vec{v}^{(0)} \in \mathbb{R}^n \) such that \( \|\vec{v}^{(0)}\|_2 = 1 \)
      </p>
      <p>
        for \( k = 1, 2, \ldots \)
      </p>
      <p>
        compute \( \lambda^{(k-1)} = [\vec{v}^{(k-1)}]^T A \vec{v}^{(k-1)} \)
      </p>
      <p>
        solve \( (A - \lambda^{(k-1)}I)\vec{w} = \vec{v}^{(k-1)} \) for \( \vec{w} \)
      </p>
      <p>
        compute \( \vec{v}^{(k)} = \vec{w} / \|\vec{w}\|_2 \).
      </p>
      <p>
        Assume \( A = \begin{bmatrix} d_1 & 0 \\ 0 & d_2 \end{bmatrix} \) with \( d_1 > d_2 \), and \( \vec{v}^{(k-1)} = [v_1^{(k-1)}, v_2^{(k-1)}]^T \) with \( v_1^{(k-1)} \neq 0 \) and \( v_2^{(k-1)} \neq 0 \).
      </p>
        <ul>
            <li>
                (a) Express components of \( \vec{v}^{(k)} = [v_1^{(k)}, v_2^{(k)}]^T \) in terms of \( v_1^{(k-1)} \) and \( v_2^{(k-1)} \).
            </li>
            <li>
                (b) Assume, in addition, that \( v_1^{(k-1)} \approx 1 \) and \( v_2^{(k-1)} \approx 0 \). Show that \( \|\vec{v}^{(k)} - \vec{e}_1\|_2 \approx \|\vec{v}^{(k-1)} - \vec{e}_1\|_2^3 \), where \( \vec{e}_1 = [1, 0]^T \). What is the significance of this result? <b>Hint:</b> \( \sqrt{1 + x} \approx 1 + x/2 \) for \( x \approx 0 \).
            </li>
        </ul>
      <p>
        <b>Solution (a).</b> For simplicity, we denote \(a = v_1^{(k-1)}\in\mathbb{R}\) and \(b = v_2^{(k-1)}\in\mathbb{R}\). Then, we have
        \[ v^{(k-1)} = [a, b]^T. \]
        Thus, we have
        \[
        \begin{align*}
        \lambda^{(k-1)} &= [v^{(k-1)}]^T A v^{(k-1)} = [a, b] \begin{bmatrix} d_1 & 0 \\ 0 & d_2 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = [a, b] \begin{bmatrix} d_1 a \\ d_2 b \end{bmatrix} = d_1 a^2 + d_2 b^2, \\
        A- \lambda^{(k-1)}I &= \begin{bmatrix} d_1 & 0 \\ 0 & d_2 \end{bmatrix} - \begin{bmatrix} d_1 a^2 + d_2 b^2 & 0 \\ 0 & d_1 a^2 + d_2 b^2 \end{bmatrix} = \begin{bmatrix} d_1 - d_1 a^2 - d_2 b^2 & 0 \\ 0 & d_2 - d_1 a^2 - d_2 b^2 \end{bmatrix}. \\
        \end{align*}
        \]
        Since, we have \(v^{(k-1)} = w/\|w\|_2\), we have the norm of \(v^{(k-1)}\) is 1. Therefore, we have \(a^2 + b^2 = 1\). Then, we have
        \[
        A- \lambda^{(k-1)}I = \begin{bmatrix} d_1(1 - a^2) - d_2b^2 & 0 \\ 0 & d_2(1 - b^2) -d_1a^2 \end{bmatrix} = \begin{bmatrix} d_1b^2 - d_2b^2 & 0 \\ 0 & d_2a^2 -d_1a^2 \end{bmatrix} = \begin{bmatrix} (d_1 - d_2)b^2 & 0 \\ 0 & (d_2 - d_1)a^2 \end{bmatrix}.
        \]
        Since, we know that \(b_1>b_2\) and neither \(a\) nor \(b\) is zero, we have \((d_1 - d_2)b^2\neq 0\) and \((d_2 - d_1)a^2\neq 0\).
        Thus, we can know that \(A - \lambda^{(k-1)}I\) is non-singular. Therefore, we have the inverse of \(A - \lambda^{(k-1)}I\) as
        \[
        (A - \lambda^{(k-1)}I)^{-1} = \begin{bmatrix} \frac{1}{(d_1 - d_2)b^2} & 0 \\ 0 & \frac{1}{(d_2 - d_1)a^2} \end{bmatrix}.
        \]
        Then, we have
        \[
        w = (A - \lambda^{(k-1)}I)^{-1}v^{(k-1)} = \begin{bmatrix} \frac{1}{(d_1 - d_2)b^2} & 0 \\ 0 & \frac{1}{(d_2 - d_1)a^2} \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} \frac{a}{(d_1 - d_2)b^2} \\ \frac{b}{(d_2 - d_1)a^2} \end{bmatrix}.
        \]
        Hence, we have
        \[
        \|w\|_2 = \sqrt{\left(\frac{a}{(d_1 - d_2)b^2}\right)^2 + \left(\frac{b}{(d_2 - d_1)a^2}\right)^2} = \sqrt{\frac{a^2}{(d_1 - d_2)^2b^4} + \frac{b^2}{(d_2 - d_1)^2a^4}} = \sqrt{\frac{a^6}{(d_1 - d_2)^2a^4b^4} + \frac{b^6}{(d_2 - d_1)^2a^4b^4}}.
        \]
        Given that \(b_1>b_2\),
        \[
        \begin{align}
        \|w\|_2 &= \sqrt{\frac{a^6 + b^6}{(d_1 - d_2)^2a^4b^4}} = \dfrac{\sqrt{a^6 + b^6}}{(d_1 - d_2)a^2b^2}, \\
        \vec{v}^{(k)} &= \dfrac{\vec{w}}{\|\vec{w}\|_2} = \dfrac{\begin{bmatrix} \frac{a}{(d_1 - d_2)b^2} \\ \frac{b}{(d_2 - d_1)a^2} \end{bmatrix}}{\dfrac{\sqrt{a^6 + b^6}}{(d_1 - d_2)a^2b^2}} = \begin{bmatrix} \dfrac{a^3}{\sqrt{a^6 + b^6}} \\ -\dfrac{b^3}{\sqrt{a^6 + b^6}} \end{bmatrix}.
        \end{align}
        \]
        Thus, we have the components of \( \vec{v}^{(k)} \) in terms of \( v_1^{(k-1)} \) and \( v_2^{(k-1)} \) as
        \[
        \begin{align}
        v_1^{(k)} &= \dfrac{(v_1^{(k)})^3}{\sqrt{(v_1^{(k)})^6 + (v_2^{(k)})^6}}, \\
        v_2^{(k)} &= -\dfrac{(v_2^{(k)})^3}{\sqrt{(v_1^{(k)})^6 + (v_2^{(k)})^6}}.
        \end{align}
        \]
      </p>
      <p>
        According to the \(LU\) factorization, we can know that the diagonal entries of \(L\) are all ones.
        We denote \(L_k, U_k\) as the \(k\)th leading principal sub-matrix of \(L, U\), respectively.
        And we can know that \(A_k = L_kU_k\). Since \(\det A_k>0\) and \(\det L_k = 1\) and \(\det A_k = \det U_k \det L_k\),
        we can know that \(\det U_k > 0\). Since \(\det U_k = u_{11} \dots u_{kk}\), we can know that \(u_{11}\cdots u_{kk} > 0\).
        Now, we want to prove that \(u_{ii}>0\) by induction.
        Since \(\det U_1 > 0\), we can know that \(u_{11} > 0\).
        Suppose that \(u_{11}\cdots u_{ii} > 0\). Then, we have \(\det U_{i+1} = u_{11}\cdots u_{ii}u_{i+1,i+1} > 0\).
        It forces that \(u_{i+1,i+1} > 0\). Therefore, we can know that the diagonal elements of \(U\) are positive.
        Thus, \(D = \text{diag}(\sqrt{u_{11}}, \dots, \sqrt{u_{mm}})\) exists and is non-singular.

        Let \(R = D^{-1}U\). Since \(D^{-1} = \text{diag}(\frac{1}{\sqrt{u_{11}}}, \dots, \frac{1}{\sqrt{u_{mm}}})\), we can know that \(D^{-1}\) is an upper triangular matrix.
        Since \(U\) is an upper triangular matrix, we can know that \(R\), which is a product of two upper triangular matrices, is an upper triangular matrix.

        Now, we want to show that \(A = R^*R\).
        Since \(A\) is Hermitian, we have \(A = A^*\).
        Let \(A = LU\), then \(A = (LU)^* = U^*L^* = LU\).
        Since the diagonal entries of \(L\) are \(U\) are positive, we can know that \(L, U\) are non-singular, which implies that \(U^*, L^*\) are also non-singular.
        Then, given \(U^*L^* = LU\), we can know that \(U(L^*)^{-1} = L^{-1}U^*\).
        Again, according to the properties of upper and lower triangular matrices, we can know that \((L^*)^{-1}\) is an upper triangular matrix and \( U^*\) is a lower triangular matrix.
        Hence, \(U(L^*)^{-1}\) is an upper triangular matrix and \(L^{-1}U^*\) is a lower triangular matrix.
        Since \(U(L^*)^{-1} = L^{-1}U^*\), we can know that \(U(L^*)^{-1} = L^{-1}U^*\) is a diagonal matrix.
        Now, we look at the entries of \(L^{-1}U^*\). Firstly, the diagonal entries of \(L^{-1}\) are still ones and the diagonal entries of \(U^*\) are the same as the diagonal entries of \(U\).
        Now, by the matrix multiplication, we see that the diagonal entries of \(L^{-1}U^*\) are \(u_{11}, \dots, u_{mm}\).
        Since itself is a diagonal matrix, we can know that
        \[
        L^{-1}U^* = \text{diag}(u_{11}, \dots, u_{mm}) = D^2.
        \]
        It shows that \(U^* = LD^2\).
        Given \(D^{-1}\) is a diagonal matrix, we can know that \((D^{-1})^* = D^{-1}\).
        \begin{align*}
        (D^{-1}U)^* &= U^*(D^{-1})^* = U^*D^{-1} = LD^2D^{-1} = LD.
        \end{align*}
        Therefore, we can see that
        \[
        R^* R = (D^{-1}U)^*(D^{-1}U) = LDD^{-1}U = LU = A. \tag*{\(\square\)}
        \]
      </p>
    </section>

    <br>

    <section id="August_2023">
      <h2>Qualify Exam Problems (August 2023)</h2>
      <p>
        \(\textbf{Problem 3. }\)Consider the \(\theta\)-method \[ y_{n+1} = y_n
        + h \left( \theta f_{n+1} + (1 - \theta) f_n \right), \quad \theta \in
        [0, 1], \] where \( f_n = f(t_n, y_n) \).
      </p>
      <ul>
        <li>
          (a). For which value(s) of \(\theta\) is the \(\theta\)-method
          explicit? Implicit?
        </li>
        <li>
          (b). Find an expression for the local truncation error in terms of
          \(\theta\), \(h\), and derivatives of the exact solution \(y(t)\) to
          the initial value problem. What are the local truncation errors for
          \(\theta = 0\), \(\frac{1}{2}\), \(1\)?
        </li>
      </ul>
      <p>
        \(\textbf{Solution (a).}\)To make a method explicit, we only need values
        of \(y_n\) and \(f_n\). Hence, when \(\theta = 0\), we have \[ y_{n+1} =
        y_n + h f_n, \] which is explicit. To make a method implicit, we need
        values of \(f_{n+1}\). Hence, when \(\theta = 1\), we have \[ y_{n+1} =
        y_n + h f_{n+1}, \] which is implicit.
      </p>
      <p>
        <b>Solution (b).</b> The local truncation error is given by
        \[\tau_{n+1} = y(t_{n+1}) - y_{n+1} - h(\theta f(t_{n+1}, y(t_{n+1})) +
        (1 - \theta) f(t_n, y(t_n))).\] When \(\theta = 0\), we have
        \[\tau_{n+1} = y(t_{n+1}) - y_n - h f(t_n, y(t_n)),\] which is the local
        truncation error for \(\theta = 0\). When \(\theta = \frac{1}{2}\), we
        have \[\tau_{n+1} = y(t_{n+1}) - y_n - \frac{h}{2} (f(t_{n+1},
        y(t_{n+1})) + f(t_n, y(t_n))),\] which is the local truncation error for
        \(\theta = \frac{1}{2}\). When \(\theta = 1\), we have \[\tau_{n+1} =
        y(t_{n+1}) - y_n - h f(t_{n+1}, y(t_{n+1})),\] which is the local
        truncation error for \(\theta = 1\).
      </p>
    </section>

    <section id="References">
      <h2>References</h2>
      <ul>
        <li>
          D. Kincard & W. Cheney "Numerical Analysis: Mathematics of Scientific Computing", AMS, Third Edition, 2009.
        </li>
        <li>
          E. S&uumlli & D. F. Mayers "An introduction to numerical analysis", Cambridge University Press, 2003.
        </li>
      </ul>
    </section>
  </body>
</html>
