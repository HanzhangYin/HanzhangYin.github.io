<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script><!--It is a link for MathJax-->
    <link rel="preconnect" href="https://fonts.googleapis.com"> <!--link for the font-->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!--link for the font-->
    <link href="https://fonts.googleapis.com/css2?family=Spectral:wght@200&display=swap" rel="stylesheet"> <!--link for the font-->
    <link rel="stylesheet" type="text/css" href="note_style.css"><!--It is a link for css structure-->
    <script src="highlight.js"></script><!--It is a link for highlight.js-->
    <title>Linear Algebra</title>
  </head>
  <body>
    <div class="top-bar"></div>
    <input type="checkbox" id="nav-toggle" class="nav-toggle">
    <label for="nav-toggle" class="icon-burger">
      <div class="line"></div>
      <div class="line"></div>
      <div class="line"></div>
      <span class="visually-hidden">Menu</span>
    </label>
    <nav class="navbar">
      <a href="#linear_dependence_lemma">Linear Dependence Lemma</a>
      <a href="#References">References</a>
    </nav>
    <script src="click.js"></script><!--It is a link for click.js, which is clicking the link the hamburger menu will disappear-->
    <h1>Linear algebra</h1>

    <section id = "linear_dependence_lemma">
    <h2>Linear Dependence Lemma</h2>
      <p>\(\textbf{Lemma 1. }\)Suppose \( v_1, \ldots, v_m \) is a linearly dependent list in \( V \). Then there exists
        \( k \in \{1, 2, \ldots, m\} \) such that
        \[
        v_k \in \text{span}(v_1, \ldots, v_{k-1}).
        \]
        Furthermore, if \( k \) satisfies the condition above and the \( k \)-th term is removed
        from \( v_1, \ldots, v_m \), then the span of the remaining list equals \(\text{span}(v_1, \ldots, v_m)\).
        (i.e. \(\text{span}(v_1, \dots, v_{k-1}, v_{k+1}, \dots, v_m) = \text{span}(v_1, \ldots, v_m)\))
      </p>
      <p>
        \(\textbf{Proof. }\)Since \(v_1,\dots, v_m\) is linearly dependent, there exist numbers
        \(a_1, \dots, a_m \in \mathbb{F}\), not all for some \(a_i\neq 0\) where \(1\leq i\leq m\), such that
        \[
        a_1v_1 + \dots + a_mv_m = 0.
        \]
        Let \(k\) be the largest index such that \(a_k \neq 0\). Then
        \[
        v_k = -\frac{a_1}{a_k}v_1 - \dots - \frac{a_{k-1}}{a_k}v_{k-1}.
        \]
        It shows that \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\). 
        Now, suppose that \(k\in \{1, \dots, m\}\) and \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\). 
        Let \(u\in \text{span}(v_1, \ldots, v_m)\). Then there exist \(b_1, \dots, b_m \in \mathbb{F}\) such that
        \[
        u = b_1v_1 + \dots + b_kv_k + \dots + b_mv_m.
        \]
        Since \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\), we have
        \[
        v_k = c_1v_1 + \dots + c_{k-1}v_{k-1}
        \]
        for some \(c_1, \dots, c_{k-1} \in \mathbb{F}\). Therefore, we can rewrite \(u\) as
        \[
        u = b_1v_1 + \dots + b_k(c_1v_1 + \dots + c_{k-1}v_{k-1}) + \dots + b_mv_m.
        \]
        Thus, there will be no \(v_k\) term for \(u\in \text{span}(v_1, \ldots, v_m)\). 
        Therefore, \(\text{span}(v_1, \dots, v_{k-1}, v_{k+1}, \dots, v_m) = \text{span}(v_1, \ldots, v_m)\). \[ \tag*{$\square$} \] 
      </p>
    </section>

    <br>

    <section id = "length_of_linearly_independent_list_length_of_spanning_list">
    <h2>length of linearly independent list â‰¤ length of spanning list</h2>
      <p>
        \(\textbf{Lemma. }\)Let \( V \) be a vector space over the field \( F \). 
        If \(V = \text{Span}(u_1, \dots, u_n)\) and \(\{v_1, \dots, v_m\}\) is a linearly independent set in \(V\), 
        then \(m \leq n\).
      </p>
    </section>

    <br>

    <sectioin id = "cayley_hamilton_theorem">
      <h2>Cayley-Hamilton Theorem</h2>
      <p>
        \(\textbf{Theorem. }\)Let \(A\) be an \(n\times n\) matrix and let \(p(\lambda) = \det(A - \lambda I)\). 
        Then \(p(A) = 0\). Moreover, if \(T\in\mathbb{L}(V)\), then \(p(T) = 0\), where 
      </p>

      <section id = "orthogonal_decomposition_theorem">
        <h2>Orthogonal Decomposition Theorem</h2>
        <p>
          \(\textbf{Theorem. }\)If \(V\) is an \(n\)-dimension vector space.
          And \(S\=\{v_1, \dots, v_n\}\) is an orthogonal list of nonzero vectors in \(V\).
          Then for any \(v\in V\), we ca write 
          \[
          v = \frac{\langle v, v_1 \rangle}{\langle v_1, v_1 \rangle}v_1 + \dots + \frac{\langle v, v_n \rangle}{\langle v_n, v_n \rangle}v_n.
          \]
        </p>
        <p>
          \(\textbf{Proof. }\)Let \(v\in V\). Since \(S\) is an orthogonal list and has \(n\) vectors, we can know that
          \(S\) is a basis. 
        </p>
      </section>
      
    <br>

    <section id = "Gram_Schmidt_orthogonalization_process">
      <h2>Gram-Schmidt orthogonalization process</h2>
      <p>
        \(\textbf{Theorem. }\)Let \(V\) be an inner product space and let \(S = \{v_1, \dots\}\) be a basis in \(V\).
        And the set \(V_k = \text{span}\{v_1, \dots, v_k\}\) for \(k\geq 1\). 
        Then there exists an orthogonal list \(\{w_1, \dots, w_k\}\) such that for each \(k\geq 1\),
        \[
        V_k = \text{span}\{w_1, \dots, w_k\}.
        \]
        and \(w_k\) is orthogonal to every vector in \(V_{k-1}\).\\
        Furthermore, this sequence is unique up to multiplying the elements by nonzero scalars.
      </p>
      <p>
        \(\textbf{Proof. }\)We construct the sequence \(\{w_1, \dots, w_k\}\) recursively.
        We start with the simple choice \(w_1 = v_1\). Now, suppose that \(w_1, \dots, w_{k-1}\) have been constructed and 
        \(\text{span}(w_1, \dots, w_{k-1}) = V_{k-1}\). We define \(w_k\) by
        \[
        w_k = v_k - \frac{\langle v_k, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 - \dots - \frac{\langle v_k, w_{k-1} \rangle}{\langle w_{k-1}, w_{k-1} \rangle}w_{k-1}.
        \]
        By the construction, we can see that each of \(w_1, \dots, w_k\) is a linear combination of \(v_1, \dots, v_k\).
        Then 
        \[
        \begin{align*}
        \langle w_i, w_j \rangle &= \langle v_k - a_1w_1 - \dots - a_{k-1}w_{k-1}, w_j\\
        &= \langle v_k, w_j \rangle - a_1\langle w_1, w_j \rangle - \dots - a_{k-1}\langle w_{k-1}, w_j \rangle\\
        &= \langle v_k, w_j \rangle - a_j\langle w_j, w_j \rangle\\
        \end{align*}
        \]
        Then \(w_k\) is orthogonal to \(w_1, \dots, w_{k-1}\).
      </p>
      <p>
        \(\textbf{Corollary. }\)Every finite dimensional inner product space has an orthonormal basis.
      </p>
      <p>
        \(\textbf{Example. }\)Let \(V = \mathbb{R}^3\), and let \(S = \{v_1, v_2, v_3\}\) be a basis for \(V\), where
        \[
        v_1 = (2, 1, 2), v_2 = (5, 4, 2), v_3 = (-1, 2, 1)
        \]
        We can use Gram-Schmidt orthogonalization process to find an orthogonal basis for \(V\).
        \[
        \begin{align*}
        w_1 &= v_1 = (2, 1, 2)\\
        w_2 &= v_2 - \frac{\langle v_2, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 = (5, 4, 2) - \frac{14}{9}(2, 1, 2) = (\frac{1}{9}, \frac{2}{9}, -\frac{4}{9})\\
        w_3 &= v_3 - \frac{\langle v_3, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 - \frac{\langle v_3, w_2 \rangle}{\langle w_2, w_2 \rangle}w_2 = (-1, 2, 1) - \frac{3}{9}(2, 1, 2) - \frac{2}{9}(\frac{1}{9}, \frac{2}{9}, -\frac{4}{9}) = (-\frac{1}{3}, \frac{4}{9}, \frac{2}{9})
        \end{align*}
        \]
        Then \(\{w_1, w_2, w_3\}\) is an orthogonal basis for \(V\).
      </p>
    </section>

    <br>
    

    <section id = conjugate_eigenvalues>
        <h2>Conjugate Eigenvalues</h2>
        If \(A\) is a real matrix nd v is an eigenvector sith complex eigen values \(\lambda\), then the complex conjugate \(v\) is an eeigenvector with 
        eigenvalue \(\lambda\). 
      </section>

    <section>
        <h2></h2>
        <p>
          Let \(T\in L(v, v)\), and suppose \(\lambda_1, \lambda_2, \dots, \lambda \lambda_2\) are 
          distinct eigenvalues of \(T\). For each \(1\leq i\leq r\), 
          set 
          \[
          E_{\lambda_i} = \{v\in V: T(v) = \lambda_i v\}
          \]
          Then\\
          1. each \(E_{\lambda_i}\) is a subspace of \(V\).\\
          2. \(E_{\lambda_1}, E_{\lambda_2}, \dots, E_{\lambda_r}\) are disjoint.\\
          3. 
        </p>
      </section>

      <br>

      <section>
        <p>
          If \lambda is an eigenvalue of the matrix A which appears exactly k times as a root of the charateristic polynomial, 
          then the dimension of the eigenspace 
          \(E_\lambda \) is at least 1 and at most \(k\). 
        </p>
      </section>
    <section id = "References">
    <h2>References</h2>
    <ul>
      <li>
        First Initial. Last name, "Book name", Publications, 
        Edition, Year.
      </li>
    </ul>
  </section>
  </body>
</html>
