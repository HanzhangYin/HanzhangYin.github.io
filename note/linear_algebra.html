<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script><!--It is a link for MathJax-->
    <link rel="preconnect" href="https://fonts.googleapis.com"> <!--link for the font-->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!--link for the font-->
    <link href="https://fonts.googleapis.com/css2?family=Spectral:wght@200&display=swap" rel="stylesheet"> <!--link for the font-->
    <link rel="stylesheet" type="text/css" href="note_style.css"><!--It is a link for css structure-->
    <script src="highlight.js"></script><!--It is a link for highlight.js-->
    <title>Linear Algebra</title>
  </head>
  <body>
    <div class="top-bar"></div>
    <input type="checkbox" id="nav-toggle" class="nav-toggle">
    <label for="nav-toggle" class="icon-burger">
      <div class="line"></div>
      <div class="line"></div>
      <div class="line"></div>
      <span class="visually-hidden">Menu</span>
    </label>
    <nav class="navbar">
      <a href="#linear_dependence_lemma">Linear Dependence Lemma</a>
      <a href="#References">References</a>
    </nav>
    <script src="click.js"></script><!--It is a link for click.js, which is clicking the link the hamburger menu will disappear-->
    <h1>Linear algebra</h1>

    <section id = "linear_dependence_lemma">
    <h2>Linear Dependence Lemma</h2>
      <p>\(\textbf{Lemma 1. }\)Suppose \( v_1, \ldots, v_m \) is a linearly dependent list in \( V \). Then there exists
        \( k \in \{1, 2, \ldots, m\} \) such that
        \[
        v_k \in \text{span}(v_1, \ldots, v_{k-1}).
        \]
        Furthermore, if \( k \) satisfies the condition above and the \( k \)-th term is removed
        from \( v_1, \ldots, v_m \), then the span of the remaining list equals \(\text{span}(v_1, \ldots, v_m)\).
        (i.e. \(\text{span}(v_1, \dots, v_{k-1}, v_{k+1}, \dots, v_m) = \text{span}(v_1, \ldots, v_m)\))
      </p>
      <p>
        \(\textbf{Proof. }\)Since \(v_1,\dots, v_m\) is linearly dependent, there exist numbers
        \(a_1, \dots, a_m \in \mathbb{F}\), not all for some \(a_i\neq 0\) where \(1\leq i\leq m\), such that
        \[
        a_1v_1 + \dots + a_mv_m = 0.
        \]
        Let \(k\) be the largest index such that \(a_k \neq 0\). Then
        \[
        v_k = -\frac{a_1}{a_k}v_1 - \dots - \frac{a_{k-1}}{a_k}v_{k-1}.
        \]
        It shows that \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\). 
        Now, suppose that \(k\in \{1, \dots, m\}\) and \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\). 
        Let \(u\in \text{span}(v_1, \ldots, v_m)\). Then there exist \(b_1, \dots, b_m \in \mathbb{F}\) such that
        \[
        u = b_1v_1 + \dots + b_kv_k + \dots + b_mv_m.
        \]
        Since \(v_k \in \text{span}(v_1, \ldots, v_{k-1})\), we have
        \[
        v_k = c_1v_1 + \dots + c_{k-1}v_{k-1}
        \]
        for some \(c_1, \dots, c_{k-1} \in \mathbb{F}\). Therefore, we can rewrite \(u\) as
        \[
        u = b_1v_1 + \dots + b_k(c_1v_1 + \dots + c_{k-1}v_{k-1}) + \dots + b_mv_m.
        \]
        Thus, there will be no \(v_k\) term for \(u\in \text{span}(v_1, \ldots, v_m)\). 
        Therefore, \(\text{span}(v_1, \dots, v_{k-1}, v_{k+1}, \dots, v_m) = \text{span}(v_1, \ldots, v_m)\). \[ \tag*{$\square$} \] 
      </p>
    </section>

    <br>

    <section id = "length_of_linearly_independent_list_length_of_spanning_list">
    <h2>length of linearly independent list â‰¤ length of spanning list</h2>
      <p>
        \(\textbf{Lemma. }\)Let \( V \) be a vector space over the field \( F \). 
        If \(V = \text{Span}(u_1, \dots, u_n)\) and \(\{v_1, \dots, v_m\}\) is a linearly independent set in \(V\), 
        then \(m \leq n\).
      </p>
    </section>

    <br>

    <sectioin id = "cayley_hamilton_theorem">
      <h2>Cayley-Hamilton Theorem</h2>
      <p>
        \(\textbf{Theorem. }\)Let \(A\) be an \(n\times n\) matrix and let \(p(\lambda) = \det(A - \lambda I)\). 
        Then \(p(A) = 0\). Moreover, if \(T\in\mathbb{L}(V)\), then \(p(T) = 0\), where 
      </p>

      <section id = "orthogonal_decomposition_theorem">
        <h2>Orthogonal Decomposition Theorem</h2>
        <p>
          \(\textbf{Theorem. }\)If \(V\) is an \(n\)-dimension vector space.
          And \(S\=\{v_1, \dots, v_n\}\) is an orthogonal list of nonzero vectors in \(V\).
          Then for any \(v\in V\), we ca write 
          \[
          v = \frac{\langle v, v_1 \rangle}{\langle v_1, v_1 \rangle}v_1 + \dots + \frac{\langle v, v_n \rangle}{\langle v_n, v_n \rangle}v_n.
          \]
        </p>
        <p>
          \(\textbf{Proof. }\)Let \(v\in V\). Since \(S\) is an orthogonal list and has \(n\) vectors, we can know that
          \(S\) is a basis. 
        </p>
      </section>
      
    <br>

    <section id = "Gram_Schmidt_orthogonalization_process">
      <h2>Gram-Schmidt orthogonalization process</h2>
      <p>
        \(\textbf{Theorem. }\)Let \(V\) be an inner product space and let \(S = \{v_1, \dots\}\) be a basis in \(V\).
        And the set \(V_k = \text{span}\{v_1, \dots, v_k\}\) for \(k\geq 1\). 
        Then there exists an orthogonal list \(\{w_1, \dots, w_k\}\) such that for each \(k\geq 1\),
        \[
        V_k = \text{span}\{w_1, \dots, w_k\}.
        \]
        and \(w_k\) is orthogonal to every vector in \(V_{k-1}\).\\
        Furthermore, this sequence is unique up to multiplying the elements by nonzero scalars.
      </p>
      <p>
        \(\textbf{Proof. }\)We construct the sequence \(\{w_1, \dots, w_k\}\) recursively.
        We start with the simple choice \(w_1 = v_1\). Now, suppose that \(w_1, \dots, w_{k-1}\) have been constructed and 
        \(\text{span}(w_1, \dots, w_{k-1}) = V_{k-1}\). We define \(w_k\) by
        \[
        w_k = v_k - \frac{\langle v_k, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 - \dots - \frac{\langle v_k, w_{k-1} \rangle}{\langle w_{k-1}, w_{k-1} \rangle}w_{k-1}.
        \]
        By the construction, we can see that each of \(w_1, \dots, w_k\) is a linear combination of \(v_1, \dots, v_k\).
        Then 
        \[
        \begin{align*}
        \langle w_i, w_j \rangle &= \langle v_k - a_1w_1 - \dots - a_{k-1}w_{k-1}, w_j\\
        &= \langle v_k, w_j \rangle - a_1\langle w_1, w_j \rangle - \dots - a_{k-1}\langle w_{k-1}, w_j \rangle\\
        &= \langle v_k, w_j \rangle - a_j\langle w_j, w_j \rangle\\
        \end{align*}
        \]
        Then \(w_k\) is orthogonal to \(w_1, \dots, w_{k-1}\).
      </p>
      <p>
        \(\textbf{Corollary. }\)Every finite dimensional inner product space has an orthonormal basis.
      </p>
      <p>
        \(\textbf{Example. }\)Let \(V = \mathbb{R}^3\), and let \(S = \{v_1, v_2, v_3\}\) be a basis for \(V\), where
        \[
        v_1 = (2, 1, 2), v_2 = (5, 4, 2), v_3 = (-1, 2, 1)
        \]
        We can use Gram-Schmidt orthogonalization process to find an orthogonal basis for \(V\).
        \[
        \begin{align*}
        w_1 &= v_1 = (2, 1, 2)\\
        w_2 &= v_2 - \frac{\langle v_2, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 = (5, 4, 2) - \frac{14}{9}(2, 1, 2) = (\frac{1}{9}, \frac{2}{9}, -\frac{4}{9})\\
        w_3 &= v_3 - \frac{\langle v_3, w_1 \rangle}{\langle w_1, w_1 \rangle}w_1 - \frac{\langle v_3, w_2 \rangle}{\langle w_2, w_2 \rangle}w_2 = (-1, 2, 1) - \frac{3}{9}(2, 1, 2) - \frac{2}{9}(\frac{1}{9}, \frac{2}{9}, -\frac{4}{9}) = (-\frac{1}{3}, \frac{4}{9}, \frac{2}{9})
        \end{align*}
        \]
        Then \(\{w_1, w_2, w_3\}\) is an orthogonal basis for \(V\).
      </p>
    </section>

    <br>
    

    <section id = conjugate_eigenvalues>
        <h2>Conjugate Eigenvalues</h2>
        If \(A\) is a real matrix nd v is an eigenvector sith complex eigen values \(\lambda\), then the complex conjugate \(v\) is an eeigenvector with 
        eigenvalue \(\lambda\). 
      </section>

    <section>
        <h2></h2>
        <p>
          Let \(T\in L(v, v)\), and suppose \(\lambda_1, \lambda_2, \dots, \lambda \lambda_2\) are 
          distinct eigenvalues of \(T\). For each \(1\leq i\leq r\), 
          set 
          \[
          E_{\lambda_i} = \{v\in V: T(v) = \lambda_i v\}
          \]
          Then\\
          1. each \(E_{\lambda_i}\) is a subspace of \(V\).\\
          2. \(E_{\lambda_1}, E_{\lambda_2}, \dots, E_{\lambda_r}\) are disjoint.\\
          3. 
        </p>
      </section>

      <br>

      <section>
        <p>
          If \lambda is an eigenvalue of the matrix A which appears exactly k times as a root of the charateristic polynomial, 
          then the dimension of the eigenspace 
          \(E_\lambda \) is at least 1 and at most \(k\). 
        </p>
      </section>
    
    <br>

    <section>
    <p>
    \(\textbf{Theorem. }\)If \(\lambda\) is an eigenvalue of the matrix \(A\) which appears \(k\) times as a root of the 
    characteristic polynomial, then the dimension of the eigenspace \(E_\lambda\) is at least 1 and at most \(k\).
    </p>
    <p>
    The statement that the \(E_\lambda\) has dimension at least 1 is equivalent to the statement that the equation
    \[
    (A - \lambda I)x = 0
    \]
    For the other statement, \(E_\lambda\) has dimension at most \(k\) is the solution space that 
    \[
    (A - \lambda I)x = 0
    \]
    or \(E_\lambda\) is the null space of \(A - \lambda I\).
    If \(\lambda\) appears \(k\) times as a root of the characteristic polynomial,
    then let \(B\) be the reduced row-echelon form of \(\lambda I - A\). Then \(B\) has at most \(k\) zero rows,
    because otherwise, the matrix \(N\) would have \(0\) as an eigen value more than \(k\) times (Since \(B\) is upper triangluar).

    Then the dimension of the null space of \(B\) is at least \(n - k\). Since the null space of \(B\) is the same as the null space of \(A - \lambda I\
    then the dimension of the null space of \(A - \lambda I\) is at least \(n - k\).
    </p>
    </section>

    <br>

    <section>
    <p>
    A linear operator \(T: V\to V\) on a finite dimensional vector space \(V\) is diagonalizable if there exists a basis 
    \(\beta\) on \(V\) such that 
    \[
    [T]_\beta^\beta
    \]
    is a diagonal matrix.
    </p>
    </section>

    <br>

    <section>
    <p>
    \(\textbf{Proposition. }\)If \(A\) and \(B\) are similar, 
    then they have the same characteristic polynomial, determinant, trace, and eigenvalues.
    </p>
    </section>


    <br>

    <section>
    <p>
    \(\textbf{Proposition. }\)If \(B = Q^{-1}AQ\), then \(v\) is an eigenvector of \(B\) 
    with eigenvalue \(\lambda\) if and only if \(Qv\) is an eigenvector of \(A\) with eigenvalue \(\lambda\).
    </p>
    <p>
    \(\textbf{Proof.}\) SInce \(Q\) is invertible, \(v=0\) if and only if \(Qv = 0\).
    We can assume that \(v\neq 0\). Suppos \(v\) is an eigenvector of \(B\) with eigenvalue \(\lambda\). Then
    \[
    A(Qv) = QBQ^{-1}(Qv) = QBv = Q\lambda v = \lambda Qv
    \]
    So \(Qv\) is an eigenvector of \(A\) with eigenvalue \(\lambda\). 
    Conversely, suppose \(Qv\) is an eigenvector of \(A\) with eigenvalue \(\lambda\). Then 
    \[
    Bv = Q^{-1}AQv = Q^{-1}\lambda(Qv) = \lambda Q^{-1}(Qv) = \lambda v.
    \]
    So \(v\) is an eigenvector of \(B\) with eigenvalue \(\lambda\).
    Then \(v\) is an eigenvector of \(B\) with eigenvalue \(\lambda\) if and only if \(Qv\) is an eigenvector of \(A\) with eigenvalue \(\lambda\).
    \(\blacksquare\)
    </p>
    </section>

    <br>

    <section>
    <p>
    \(\textbf{Theorem. }\)A linear operator \(T: V\to V\) on a finite dimensional vector space \(V\) is diagonalizable if and only if
    there exists a basis \(\beta\) of \(v\) consisting of eigenvector of \(T\).
    </p>
    <p>
    \(\textbf{Proof. }\)Suppose that \(V\) has a basis of eigenvectors 
    \(\beta = \{v_1, v_2, \dots, v_n\}\) corresponding eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\).
    Then by definition, \(T(v_i) = \lambda_i v_i\) for each \(i\). 
    So the matrix of \(T\) with respect to \(\beta\) is the diagonal matrix
    \[
    [T]_\beta^\beta =
    \begin{bmatrix}
      \lambda_1 & 0 & \dots & 0\\
      0 & \lambda_2 & \dots & 0\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \dots & \lambda_n
    \end{bmatrix}
    \]
    So \(T\) is diagonalizable.
    Conversely, suppose that \(T\) is diagonalizable. Then there exists a basis \(\beta\) of \(V\) such that 
    \([T]_beta^beta\) is a diagonal matrix, whose \(\lambda_1, \lambda_2, \dots, \lambda_n\) be the diagonal entries.
    </p>
    </section>
    
    <br>

    <section>
    <h2>Independence of eigenvector</h2>
    <p>
    \(\textbf{Theorem. }\)If \(v_1, v_2, \dots, v_n\) are eigenvectors of \(T\) conssited to distinct eigenvalues 
    \(\lambda_1, \lambda_2, \dots, \lambda_n\), then \(v_1, v_2, \dots, v_n\) are linearly independent.
    </p>
    <p>
    We induct on \(n\). It is true that \(n = 1\). Suppose that \(n-1\geq 2\) case is true, and we have
    \[
    a_1v_1 + a_2v_2 + \dots + a_nv_n = 0
    \]
    We apply \(T\) to both sides of the equation, and we get
    \[
    a_1\lambda_1v_1 + a_2\lambda_2v_2 + \dots + a_n\lambda_nv_n = 0
    \]
    \[
    a_1\lambda_1v_1 + a_2\lambda_1v_2 + \dots + a_{n-1}\lambda_{1}v_{n-1} = 0
    \]
    based on the first equation.
    Then we get
    \[


    </p>
    </section>

    <br>

    <section>
    <h2>Split Completely over \(\mathbb{F}\)</h2>
    <p>
    \(\textbf{Definition. }\)A polynomial \(f(x)\) splits completely over a field \(\mathbb{F}\) if
    \[
    f(x) = a(x - \lambda_1)(x - \lambda_2)\dots(x - \lambda_n)
    \]
    where \(a, \lambda_1, \lambda_2, \dots, \lambda_n \in \mathbb{F}\).
    </p>
    </section>

    <br>

    <section>
    <h2>Diagonlization criterion</h2>
    <p>
    \(\textbf{Theorem. }\) A matrix \(A\in M_n(\mathbb{F})\) is 
    diagonalizable if and only if \(A\) has all it s eigenvalues in \(\mathbb{F}\) and
    for each eigenvalue \(\lambda\) of \(A\), the dimension of the eigenspace \(E_\lambda\) equals the multiplicity of \(\lambda\) as a root of the characteristic polynomial of \(A\).
    </p>
    <p>
    \(\textbf{Proof. }\)Suppose that \(A\) is diagonalizable.
    Then the diagonal entries of its diagnolized matrix are eigenvalues of \(A\), 
    so they all lies in \(\mathbb{F}\).
    By our pervous theorme on diagonalizability, 
    \(V\) has a basis \(\beta\) of eigenvectors of \(A\). 
    For any eigenvalues \(\lambda_i\) of \(A\),
    let \(b_i\) be the number of elements of \(\beta\) 
    having eigenvalues \(\lambda_i\). 
    and let \(d_i\) be the multiplicity of \(\lambda_i\) as a root of the characteristic polynomial of \(A\).
    Then: \(\sum_i b_i = n\) since \(\beta\) is a basis for \(V\), and 
    \(\sum_{i}d_i = n\) since the characteristic polynomial of \(A\) has degree \(n\).
    Since the characteristic polynomial of \(A\) is the product of the linear factors \((x - \lambda_i)^{d_i}\),
    Thus, \(b_i \leq d_i\) for each \(i\). Since \(\sum_i b_i = \sum_i d_i\), we have \(b_i = d_i\) for each \(i\).\\
    For the other direction, suppose that all eigenvalues of \(A\) lie in \(\mathbb{F}\) and for each eigenvalue \(\lambda\) of \(A\), 
    the dimension of the eigenspace \(E_\lambda\) equal the multiplicity of \(\lambda\) as a root of the characteristic polynomial of \(A\).
    Then let \(\beta\) be the union of bases for the eigenspaces of \(A\). 
    By hypothesis, \(\beta\) has \(n\) elements.
    So to cluce it is a basiss for the \(n\)-dimensional space \(V\), 
    we need only show it is linearly indepenedent.
    Let \(v_1, v_2, \dots, v_n\) be the basis of the \(E_{\lambda_i}\) for each \(i\).
  </p>


    <section id = "References">
    <h2>References</h2>
    <ul>
      <li>
        First Initial. Last name, "Book name", Publications, 
        Edition, Year.
      </li>
    </ul>
  </section>
  </body>


</html>
