<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
    ></script>
    <!--It is a link for MathJax-->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <!--link for the font-->
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <!--link for the font-->
    <link
      href="https://fonts.googleapis.com/css2?family=Spectral:wght@200&display=swap"
      rel="stylesheet"
    />
    <!--link for the font-->
    <link rel="stylesheet" type="text/css" href="note_style.css" />
    <!--It is a link for css structure-->
    <title>Some Theorems in Algebra</title>
  </head>
  <body>
    <div class="top-bar"></div>
    <input type="checkbox" id="nav-toggle" class="nav-toggle" />
    <label for="nav-toggle" class="icon-burger">
      <div class="line"></div>
      <div class="line"></div>
      <div class="line"></div>
      <span class="visually-hidden">Menu</span>
    </label>
    <nav class="navbar">
      <a href="#References">References</a>
    </nav>
    <script src="click.js"></script>
    <!--It is a link for click.js, which is clicking the link the hamburger menu will disappear-->
    <script src="highlight.js"></script>
    <!--It is a link for highlight.js-->
    <h1>Probability and Statistics</h1>

    <section>
      <p>
        My name is Hanzhang Yin, and I have developed this website as a resource
        to facilitate the review of key concepts in abstract algebra and it is
        not fully completed. Feel free to send me an email at
        <a href="mailto:hanyin@ku.edu">hanyin@ku.edu</a> if there are any errors
        or suggestions for improvement.
      </p>
    </section>

    <section>
      <h2>January 2023</h2>
      <p>
        \(\textbf{S 2.}\) Consider a random sample from the density \( f_\theta(x) = \frac{3 \theta^3}{(x + \theta)^4} \), \( x > 0 \), where \( \theta > 0 \) is a parameter. Calculate the efficiency of the statistic defined as two times the sample mean.
      </p>
      <p>
        \(\textbf{Solution. }\)Firstly, we try to find \(E[X]\) and \(Var[X]\). The expectation of \(X\) is
        \[
        \begin{align}
        E[X] &= \int_{0}^{\infty} x \cdot \frac{3\theta^3}{(x+\theta)^4} \, dx \\
        &= 3\theta^3 \int_{0}^{\infty} \frac{x}{(x+\theta)^4} \, dx \\
        &= 3\theta^3 \int_{0}^{\infty} \frac{u-\theta}{u^4} \, du \quad \text{where } u = x + \theta \\
        &= 3\theta^3 \int_{\theta}^{\infty} \frac{u-\theta}{u^4} \, du \\
        &= 3\theta^3 \left[ \int_{\theta}^{\infty} \frac{1}{u^3} \, du - \theta \int_{\theta}^{\infty} \frac{1}{u^4} \, du \right] \\
        &= 3\theta^3 \left[ \left. -\frac{1}{2u^2} \right|_{\theta}^{\infty} - \theta \left( -\frac{1}{3u^3} \right)\Bigg|_{\theta}^{\infty} \right] \\
        &= 3\theta^3 \left[ 0 - \left( -\frac{1}{2\theta^2} \right) - \theta \left( 0 - \left( -\frac{1}{3\theta^3} \right) \right) \right] \\
        &= 3\theta^3 \left[ \frac{1}{2\theta^2} + \frac{1}{3\theta^2} \right] \\
        &= 3\theta^3 \left[ \frac{5}{6\theta^2} \right] \\
        &= \frac{5}{2}\theta.
        \end{align}
        \]
        Then, we try to find \(E[X^2]\).
        \[
        \begin{align}
        E[X^2] &= \int_{0}^{\infty} x^2 \cdot \frac{3\theta^3}{(x+\theta)^4} \, dx \\
        &= 3\theta^3 \int_{0}^{\infty} \frac{x^2}{(x+\theta)^4} \, dx \\
        &= 3\theta^3 \int_{0}^{\infty} \frac{u^2-2\theta u + \theta^2}{u^4} \, du \quad \text{where } u = x + \theta \\
        &= 3\theta^3 \int_{\theta}^{\infty} \frac{u^2-2\theta u + \theta^2}{u^4} \, du \\
        &= 3\theta^3 \left[ \int_{\theta}^{\infty} \frac{1}{u^2} \, du - 2\theta \int_{\theta}^{\infty} \frac{1}{u^3} \, du + \theta^2 \int_{\theta}^{\infty} \frac{1}{u^4} \, du \right] \\
        &= 3\theta^3 \left[ \left. -\frac{1}{u} \right|_{\theta}^{\infty} - \theta \left( -\frac{1}{u^2} \right)\Bigg|_{\theta}^{\infty} + \theta^2 \left( -\frac{1}{3u^3} \right)\Bigg|_{\theta}^{\infty} \right] \\
        &= 3\theta^3 \left[ 0 - \left( -\frac{1}{\theta} \right) - \theta \left( 0 - \left( -\frac{1}{\theta^2} \right) \right) + \theta^2 \left( 0 - \left( -\frac{1}{3\theta^3} \right) \right) \right] \\
        &= 3\theta^3 \left[ \frac{1}{\theta} + \frac{1}{\theta} + \frac{1}{3\theta} \right] \\
        &= 3\theta^3 \left[ \frac{7}{3\theta} \right] \\
        &= 7\theta^2.
        \end{align}
        \]
        Finally, we find \(Var[X]\).
        \[
        \begin{align}
        Var[X] &= E[X^2] - (E[X])^2 \\
        &= 7\theta^2 - \left( \frac{5}{2}\theta \right)^2 \\
        &= 7\theta^2 - \frac{25}{4}\theta^2 \\
        &= \frac{28}{4}\theta^2 - \frac{25}{4}\theta^2 \\
        &= \frac{3}{4}\theta^2.
        \end{align}
        \]
        Now, we find the fisher information.
        \[
        \begin{align}
        I(\theta) &= -E\left[ \frac{\partial^2}{\partial \theta^2} \log f_\theta(X) \right] \\
        &= -E\left[ \frac{\partial^2}{\partial \theta^2} \log \left( \frac{3\theta^3}{(X+\theta)^4} \right) \right] \\
        &= -E\left[ \frac{\partial^2}{\partial \theta^2} \left( \log 3 + 3\log\theta - 4\log(X+\theta) \right) \right] \\
        &= -E\left[ \frac{\partial}{\partial \theta} \left( \frac{3}{\theta} - \frac{4}{X+\theta} \right) \right] \\
        &= -E\left[ -\frac{3}{\theta^2} + \frac{4}{(X+\theta)^2} \right] \\
        &= -\left( -\frac{3}{\theta^2} + 4E\left[ \frac{1}{(X+\theta)^2} \right] \right) \\
        &= \frac{3}{\theta^2} - 4E\left[ \frac{1}{(X+\theta)^2} \right].
        \end{align}
        \]
        Set \(Y = \frac{1}{(X+\theta)^2}\), then \(X = \frac{1}{\sqrt{Y}} - \theta\). The density of \(Y\) is
        \[
        \begin{align}
        f_Y(y) &= f_X\left( \frac{1}{\sqrt{y}} - \theta \right) \left| \frac{d}{dy} \left( \frac{1}{\sqrt{y}} - \theta \right) \right| \\
        &= \frac{3\theta^3}{\left( \frac{1}{\sqrt{y}} - \theta + \theta \right)^4} \left| -\frac{1}{2}y^{-\frac{3}{2}} \right| \\
        &= 3\theta^3y^2\cdot \frac{1}{2}y^{-\frac{3}{2}} \\
        &= \frac{3}{2}\theta^3y^{\frac{1}{2}} \\
        &= \frac{3}{2}\frac{\theta^3}{x + \theta}.
        \end{align}
        \]
        Then, we have \(E[Y]\).
        \[
        \begin{align}
        E\left[\frac{1}{(x + \theta)^2}\right] &= \int_{0}^{\infty} \frac{1}{(x + \theta)^2}\cdot \frac{3}{2}\frac{\theta^3}{x + \theta} \, dx \\
        &= \frac{3}{2}\theta^3 \int_{0}^{\infty} \frac{1}{(x + \theta)^3} \, dx \\
        &= \frac{3}{2}\theta^3 \int_{0}^{\infty} (x + \theta)^{-3} \, dx \\
        &= \frac{3}{2}\theta^3 \left[ -\frac{1}{2(x + \theta)^2} \right]_{0}^{\infty} \\
        &= \frac{3}{2}\theta^3 \left[ 0 - \left( -\frac{1}{2\theta^2} \right) \right] \\
        &= \frac{3}{4}\theta.
        \end{align}
        \]
        Finally, we find \(I(\theta)\).
        \[
        \begin{align}
        I(\theta) &= \frac{3}{\theta^2} - 4E\left[ \frac{1}{(X+\theta)^2} \right] \\
        &= \frac{3}{\theta^2} - 4\left( \frac{3}{4}\theta \right) \\
        &= \frac{3}{\theta^2} - 3\theta.
        \end{align}
        \]
        Now, we have the sample mean \( \overline{X} = \dfrac{1}{n}\sum\limits_{i=1}^{n}X_i \) and the efficiency of the statistic defined as two times the sample mean is
        \[
        \begin{align}
        \text{Var}\left[ 2\overline{X} \right] &= 4\text{Var}\left[ \overline{X} \right] \\
        &= 4\left( \frac{1}{n^2} \sum_{i=1}^{n} \text{Var}\left[ X_i \right] \right) \\
        &= 4\left( \frac{1}{n^2} \sum_{i=1}^{n} \frac{3}{4}\theta^2 \right) \\
        &= 4\left( \frac{1}{n^2} \cdot n \cdot \frac{3}{4}\theta^2 \right) \\
        &= \frac{3\theta^2}{n}.
        \end{align}
        \]
        \[
        \begin{align}
        E\left[ 2\overline{X} \right] &= 2E\left[ \overline{X} \right] \\
        &= 2E\left[ \frac{1}{n}\sum_{i=1}^{n}X_i \right] \\
        &= 2\frac{1}{n}\sum_{i=1}^{n}E\left[ X_i \right] \\
        &= 2\frac{1}{n}\sum_{i=1}^{n}\frac{5}{2}\theta \\
        &= 5\theta.
        \end{align}
        \]
        Thus, it is not a unbiased estimator. Then, we have the efficiency of the statistic defined as two times the sample mean is
        \[
        \begin{align}
        \text{Efficiency} &= \frac{E'\left[2\overline{X}\right]}{nI(\theta)} \\
        &= \frac{5}{n\left( \frac{3}{\theta^2} - 3\theta \right)} \\
        \end{align}
        \]
      </p>
    </section>

    <section>
      <p>
        \(\textbf{Problem 1. }\) Consider a random sample of size \( n \) from a
        uniform distribution on \([ \theta, \theta^2 ]\), where \( \theta > 1 \)
        is a parameter. Find the maximum likelihood estimator of the parameter
        \( \theta \).
      </p>
      <p>
        \(\textbf{Solution. }\) Given that the random sample is from a uniform
        distribution on \([ \theta, \theta^2 ]\), the probability density
        function is \[ f(x; \theta) = \begin{cases} \frac{1}{\theta^2 - \theta}
        & \text{if } \theta \leq x \leq \theta^2, \\ 0 & \text{otherwise}.
        \end{cases} \] Thus, we have the likelihood function \[ \begin{align}
        L(\theta) &= \prod_{i=1}^{n} f(x_i; \theta) = \begin{cases} \left(
        \frac{1}{\theta^2 - \theta} \right)^n & \text{if } \theta \leq x_{(1)}
        \leq \cdots \leq x_{(n)} \leq \theta^2, \\ 0 & \text{otherwise}.
        \end{cases}\\ &= \left( \frac{1}{\theta^2 - \theta} \right)^n
        \prod_{i=1}^{n} I_{[ \theta, \theta^2 ]}(x_i), \end{align} \]
      </p>
    </section>

    <section id="jan_2024">
      <h2>January 2024</h2>
      <p>
        \(\textbf{S 1.}\) Let \( X_1, X_2, \cdots, X_n \) denote a random sample
        of size \( n > 2 \) from a distribution with the probability density
        function \[ f(x; \theta) = \begin{cases} \frac{2 \theta^2}{x^5}
        e^{-\frac{\theta}{x^2}} & \text{for } x > 0 \\ 0 & \text{elsewhere},
        \end{cases} \] where \( \theta > 0 \) is a parameter. Find the maximum
        likelihood estimator of \( \theta \).
      </p>
      <p>
        \(\textbf{Solution. }\) The likelihood function is 
        \[
        \begin{align}
        L(\theta) = \prod_{i=1}^{n} f(x_i; \theta) &= \prod_{i=1}^{n}\dfrac{2\theta^2}{x_i^5} e^{-\frac{\theta}{x_i^2}} \\
        &= 2^n \theta^{2n}\prod_{i=1}^{n} \dfrac{1}{x_i^5} e^{-\frac{\theta}{x_i^2}} \\
        &= 2^n \theta^{2n} \prod_{i=1}^{n} x_i^{-5} \exp\left[-\theta\sum_{i=1}^n\frac{1}{x_i^2}\right].
        \end{align}
        \]
        The log-likelihood function is
        \[
        \begin{align}
        l(\theta) &= \log L(\theta) = n\log 2 + 2n\log\theta - 5\sum_{i=1}^n\log x_i - \theta\sum_{i=1}^n\frac{1}{x_i^2}. \\
        \frac{\partial l(\theta)}{\partial \theta} &= \dfrac{2n}{\theta} - \sum_{i=1}^n\frac{1}{x_i^2}.
        \end{align}
        \]
        We set the derivative of the log-likelihood function to zero to find the maximum likelihood estimator of \( \theta \). 
        Thus, we have 
        \[
        \begin{align}
        \dfrac{2n}{\hat{\theta}} - \sum_{i=1}^n\frac{1}{x_i^2} &= 0 \\
        \hat{\theta} &= \dfrac{2n}{\sum_{i=1}^n\frac{1}{x_i^2}}.
        \end{align}
        \]
      </p>
      <br>
      <p>
        \(\textbf{S 2. }\) Consider a random sample from a binomial distribution \( B(n, \theta) \), where \( n \geq 2 \) is a fixed integer and \( \theta \in (0,1) \) is a parameter. To make your computations easier let us assume that the sample size is 2. Find the minimum variance unbiased estimator of \( \theta^2 \).
      </p>
      <p>
      \(\textbf{Solution. }\)Firstly, we know that 
      \[
      \begin{align}
        E[X] &= \sum_{x=0}^{2} x \binom{2}{x} \theta^x(1-\theta)^{2-x} \\
        &= 0 \binom{2}{0} \theta^0(1-\theta)^{2-0} + 1 \binom{2}{1} \theta^1(1-\theta)^{2-1} + 2 \binom{2}{2} \theta^2(1-\theta)^{2-2} \\
        &= 2\theta. \\
        \frac12E[X_1 + X_2] &= 2\theta.\\
        E[X^2] &= \sum_{x=0}^{2} x^2 \binom{2}{x} \theta^x(1-\theta)^{2-x} \\
        &= 0^2 \binom{2}{0} \theta^0(1-\theta)^{2-0} + 1^2 \binom{2}{1} \theta^1(1-\theta)^{2-1} + 2^2 \binom{2}{2} \theta^2(1-\theta)^{2-2} \\
        &= 2\theta + 2\theta^2.\\ 
        \frac12 E[X_1^2 + X_2^2] &= 2\theta + 2\theta^2.\\
        \frac14 E[X_1^2 + X^2 - X_1 - X_2] &= \theta^2.
      \end{align}
      \]
      Hence, we can see that \( \frac14 (X_1^2 + X^2 - X_1 - X_2) \) is an unbiased estimator of \( \theta^2 \).
      The fisher information is
      \[
      \begin{align}
        I(\theta) &= -E\left[ \frac{\partial^2}{\partial \theta^2} \log f_\theta(X) \right] \\
        &= -E\left[ \frac{\partial^2}{\partial \theta^2} \log \binom{2}{x} \theta^x(1-\theta)^{2-x} \right] \\
        &= -E\left[ \frac{\partial^2}{\partial \theta^2} \left( \log \binom{2}{x} + x\log\theta + (2-x)\log(1-\theta) \right) \right] \\
        &= -E\left[ \frac{\partial}{\partial \theta} \left( \frac{x}{\theta} - \frac{2-x}{1-\theta} \right) \right] \\
        &= -E\left[ -\frac{x}{\theta^2} - \frac{2-x}{(1-\theta)^2} \right] \\
        &= E\left(\frac{x}{\theta^2}\right) + E\left(\frac{2-x}{(1-\theta)^2} \right) \\
        &= \frac{2}{\theta} + \frac{2}{1-\theta} \\
        &= \frac{2}{\theta(1-\theta)}.
      \end{align}
      \]
      Now, let \(Y = \frac14 (X_1^2 + X_2^2 - X_1 - X_2)\), then 
      \[
      \begin{align} 
      \text{Var}[Y] &= \frac{1}{16} \text{Var}[X_1^2 + X_2^2 - X_1 - X_2] \\
      &= \frac{1}{16} \left( \text{Var}[X_1^2] + \text{Var}[X_2^2] + \text{Var}[X_1] + \text{Var}[X_2] \right) \\
      \end{align}
      \]
      Now, we try to find the \(E[X^4]\).
      \[
      \begin{align}
        E[X^4] &= \sum_{x=0}^{2} x^4 \binom{2}{x} \theta^x(1-\theta)^{2-x} \\
        &= 0^4 \binom{2}{0} \theta^0(1-\theta)^{2-0} + 1^4 \binom{2}{1} \theta^1(1-\theta)^{2-1} + 2^4 \binom{2}{2} \theta^2(1-\theta)^{2-2} \\
        &= 2\theta + 14\theta^2.\\
        \text{Var}[X^2] &= E[X^4] - (E[X^2])^2 \\
        &= 2\theta + 14\theta^2 - (2\theta + 2\theta^2)^2 \\
      \end{align}
      \]
      </p>
    </section>

    <section id="References">
      <h2>References</h2>
      <ul>
        <li>
          <a href="https://archive.math.ksu.edu/course?course=qe_algebra"
            >Qualifying Exams Kansas State Mathematics Ph.D. Program (Algebra
            Archive)</a
          >
        </li>
      </ul>
    </section>
  </body>
</html>
